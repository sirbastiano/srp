{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1861b81",
   "metadata": {},
   "source": [
    "# Time Series Analysis with SARPYX\n",
    "\n",
    "This notebook demonstrates advanced time series analysis capabilities in SARPYX for monitoring ground deformation over time.\n",
    "\n",
    "## Overview\n",
    "\n",
    "Time series InSAR analysis enables:\n",
    "- Long-term deformation monitoring\n",
    "- Separation of different deformation signals\n",
    "- Atmospheric artifact removal\n",
    "- Persistent scatterer identification\n",
    "- Trend and seasonal component analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95da9b5",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3257490e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "from scipy import signal, stats\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# SARPYX time series modules\n",
    "from sarpyx.science.timeseries import (\n",
    "    TimeSeriesAnalyzer,\n",
    "    PersistentScattererProcessor,\n",
    "    AtmosphericCorrector,\n",
    "    DeformationModeler\n",
    ")\n",
    "from sarpyx.utils import setup_logging\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set up logging\n",
    "setup_logging(level='INFO')\n",
    "\n",
    "print(\"✓ SARPYX Time Series modules imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8f84a9",
   "metadata": {},
   "source": [
    "## Generate Synthetic Time Series Data\n",
    "\n",
    "For demonstration, we'll create realistic synthetic InSAR time series data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a0eeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate time series parameters\n",
    "start_date = datetime(2020, 1, 1)\n",
    "end_date = datetime(2023, 12, 31)\n",
    "acquisition_interval = 12  # days (Sentinel-1 repeat cycle)\n",
    "\n",
    "# Generate acquisition dates\n",
    "dates = []\n",
    "current_date = start_date\n",
    "while current_date <= end_date:\n",
    "    dates.append(current_date)\n",
    "    current_date += timedelta(days=acquisition_interval)\n",
    "\n",
    "n_acquisitions = len(dates)\n",
    "time_years = np.array([(d - start_date).days / 365.25 for d in dates])\n",
    "\n",
    "print(f\"Generated {n_acquisitions} acquisition dates from {start_date.date()} to {end_date.date()}\")\n",
    "print(f\"Time span: {time_years[-1]:.1f} years\")\n",
    "\n",
    "# Scene parameters\n",
    "scene_width, scene_height = 100, 80\n",
    "n_pixels = scene_width * scene_height\n",
    "\n",
    "print(f\"Scene dimensions: {scene_width} x {scene_height} pixels ({n_pixels:,} total)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f772b5",
   "metadata": {},
   "source": [
    "### Create Realistic Deformation Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539896d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_deformation_components(x, y, time_years):\n",
    "    \"\"\"\n",
    "    Generate realistic deformation components:\n",
    "    - Linear trend (tectonic/subsidence)\n",
    "    - Seasonal variation (thermal expansion, hydrological)\n",
    "    - Nonlinear trend (exponential decay, acceleration)\n",
    "    - Episodic events (earthquakes, sudden changes)\n",
    "    \"\"\"\n",
    "    n_times = len(time_years)\n",
    "    \n",
    "    # 1. Linear trend - varies spatially\n",
    "    # Subsidence bowl in the center\n",
    "    center_x, center_y = scene_width // 2, scene_height // 2\n",
    "    distance_from_center = np.sqrt((x - center_x)**2 + (y - center_y)**2)\n",
    "    max_subsidence_rate = -15  # mm/year at center\n",
    "    \n",
    "    linear_rate = max_subsidence_rate * np.exp(-distance_from_center**2 / (2 * 20**2))\n",
    "    linear_component = np.outer(linear_rate, time_years)\n",
    "    \n",
    "    # 2. Seasonal variation\n",
    "    seasonal_amplitude = 5.0 * np.exp(-distance_from_center**2 / (2 * 30**2))\n",
    "    seasonal_component = np.outer(seasonal_amplitude, \n",
    "                                 np.sin(2 * np.pi * time_years + np.pi/4))\n",
    "    \n",
    "    # 3. Nonlinear component (exponential)\n",
    "    nonlinear_amplitude = -3.0 * np.exp(-distance_from_center**2 / (2 * 15**2))\n",
    "    nonlinear_component = np.outer(nonlinear_amplitude, \n",
    "                                  (1 - np.exp(-time_years / 2)))\n",
    "    \n",
    "    # 4. Episodic event (simulated earthquake at t=2.5 years)\n",
    "    event_time = 2.5\n",
    "    event_amplitude = -8.0 * np.exp(-distance_from_center**2 / (2 * 25**2))\n",
    "    event_component = np.outer(event_amplitude, \n",
    "                              (time_years > event_time).astype(float))\n",
    "    \n",
    "    return linear_component, seasonal_component, nonlinear_component, event_component\n",
    "\n",
    "# Generate coordinate grids\n",
    "x_coords, y_coords = np.meshgrid(range(scene_width), range(scene_height))\n",
    "\n",
    "# Generate deformation components for all pixels\n",
    "print(\"Generating deformation components...\")\n",
    "linear_def, seasonal_def, nonlinear_def, event_def = generate_deformation_components(\n",
    "    x_coords, y_coords, time_years\n",
    ")\n",
    "\n",
    "# Combine all components\n",
    "true_deformation = linear_def + seasonal_def + nonlinear_def + event_def\n",
    "\n",
    "print(\"✓ Deformation components generated\")\n",
    "print(f\"  Linear trend range: {np.min(linear_def[:, -1]):.1f} to {np.max(linear_def[:, -1]):.1f} mm\")\n",
    "print(f\"  Seasonal amplitude range: {np.min(seasonal_def):.1f} to {np.max(seasonal_def):.1f} mm\")\n",
    "print(f\"  Event magnitude range: {np.min(event_def):.1f} to {np.max(event_def):.1f} mm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c31f30",
   "metadata": {},
   "source": [
    "### Add Atmospheric and Noise Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8483f13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_atmospheric_effects(scene_width, scene_height, n_times):\n",
    "    \"\"\"\n",
    "    Generate realistic atmospheric phase delay patterns\n",
    "    \"\"\"\n",
    "    # Stratified atmosphere (elevation-dependent)\n",
    "    # Simulate topography\n",
    "    x = np.linspace(0, 10, scene_width)\n",
    "    y = np.linspace(0, 8, scene_height)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    elevation = 200 + 300 * np.sin(np.pi * X / 10) * np.cos(np.pi * Y / 8)\n",
    "    \n",
    "    # Atmospheric delay coefficient (varies by acquisition)\n",
    "    atm_coeffs = 0.5 + 1.5 * np.random.randn(n_times)  # mm per 100m elevation\n",
    "    \n",
    "    # Calculate stratified delays\n",
    "    stratified_atm = np.zeros((scene_height, scene_width, n_times))\n",
    "    for i in range(n_times):\n",
    "        stratified_atm[:, :, i] = atm_coeffs[i] * elevation / 100\n",
    "    \n",
    "    # Turbulent atmosphere (spatial correlation)\n",
    "    turbulent_atm = np.zeros((scene_height, scene_width, n_times))\n",
    "    for i in range(n_times):\n",
    "        # Generate correlated noise\n",
    "        noise = np.random.randn(scene_height, scene_width)\n",
    "        # Apply Gaussian filter for spatial correlation\n",
    "        from scipy.ndimage import gaussian_filter\n",
    "        correlated_noise = gaussian_filter(noise, sigma=5)\n",
    "        turbulent_atm[:, :, i] = 3.0 * correlated_noise\n",
    "    \n",
    "    return stratified_atm, turbulent_atm\n",
    "\n",
    "# Generate atmospheric effects\n",
    "print(\"Generating atmospheric effects...\")\n",
    "stratified_atm, turbulent_atm = generate_atmospheric_effects(\n",
    "    scene_width, scene_height, n_acquisitions\n",
    ")\n",
    "\n",
    "total_atmosphere = stratified_atm + turbulent_atm\n",
    "\n",
    "# Add measurement noise\n",
    "noise_std = 2.0  # mm standard deviation\n",
    "measurement_noise = noise_std * np.random.randn(\n",
    "    scene_height, scene_width, n_acquisitions\n",
    ")\n",
    "\n",
    "# Create observed time series (reshape for easier handling)\n",
    "observed_ts = true_deformation + total_atmosphere + measurement_noise\n",
    "\n",
    "print(\"✓ Atmospheric effects and noise added\")\n",
    "print(f\"  Stratified atmosphere range: ±{np.std(stratified_atm):.1f} mm\")\n",
    "print(f\"  Turbulent atmosphere range: ±{np.std(turbulent_atm):.1f} mm\")\n",
    "print(f\"  Measurement noise: {noise_std:.1f} mm std\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19583728",
   "metadata": {},
   "source": [
    "## Visualize the Synthetic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171ea961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot overview of the synthetic dataset\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Time indices for visualization\n",
    "mid_time = n_acquisitions // 2\n",
    "end_time = n_acquisitions - 1\n",
    "\n",
    "# True deformation at different times\n",
    "im1 = axes[0, 0].imshow(true_deformation[:, :, mid_time], cmap='RdBu_r')\n",
    "axes[0, 0].set_title(f'True Deformation\\n{dates[mid_time].strftime(\"%Y-%m-%d\")}')\n",
    "plt.colorbar(im1, ax=axes[0, 0], label='Displacement (mm)')\n",
    "\n",
    "im2 = axes[0, 1].imshow(true_deformation[:, :, end_time], cmap='RdBu_r')\n",
    "axes[0, 1].set_title(f'True Deformation\\n{dates[end_time].strftime(\"%Y-%m-%d\")}')\n",
    "plt.colorbar(im2, ax=axes[0, 1], label='Displacement (mm)')\n",
    "\n",
    "# Atmospheric effects\n",
    "im3 = axes[0, 2].imshow(total_atmosphere[:, :, mid_time], cmap='RdBu_r')\n",
    "axes[0, 2].set_title(f'Atmospheric Effects\\n{dates[mid_time].strftime(\"%Y-%m-%d\")}')\n",
    "plt.colorbar(im3, ax=axes[0, 2], label='Atmospheric delay (mm)')\n",
    "\n",
    "# Observed data\n",
    "im4 = axes[1, 0].imshow(observed_ts[:, :, mid_time], cmap='RdBu_r')\n",
    "axes[1, 0].set_title(f'Observed Data\\n{dates[mid_time].strftime(\"%Y-%m-%d\")}')\n",
    "plt.colorbar(im4, ax=axes[1, 0], label='Displacement (mm)')\n",
    "\n",
    "im5 = axes[1, 1].imshow(observed_ts[:, :, end_time], cmap='RdBu_r')\n",
    "axes[1, 1].set_title(f'Observed Data\\n{dates[end_time].strftime(\"%Y-%m-%d\")}')\n",
    "plt.colorbar(im5, ax=axes[1, 1], label='Displacement (mm)')\n",
    "\n",
    "# Linear velocity map\n",
    "velocity_map = (true_deformation[:, :, -1] - true_deformation[:, :, 0]) / time_years[-1]\n",
    "im6 = axes[1, 2].imshow(velocity_map, cmap='RdBu_r')\n",
    "axes[1, 2].set_title('True Linear Velocity')\n",
    "plt.colorbar(im6, ax=axes[1, 2], label='Velocity (mm/year)')\n",
    "\n",
    "# Remove axis ticks for cleaner look\n",
    "for ax in axes.flatten():\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Dataset overview:\")\n",
    "print(f\"  Scene size: {scene_width} x {scene_height} pixels\")\n",
    "print(f\"  Time series length: {n_acquisitions} acquisitions\")\n",
    "print(f\"  Maximum deformation: {np.max(true_deformation):.1f} mm\")\n",
    "print(f\"  Minimum deformation: {np.min(true_deformation):.1f} mm\")\n",
    "print(f\"  Maximum velocity: {np.max(velocity_map):.1f} mm/year\")\n",
    "print(f\"  Minimum velocity: {np.min(velocity_map):.1f} mm/year\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbf0bdb",
   "metadata": {},
   "source": [
    "## Time Series Analysis: Point Examples\n",
    "\n",
    "Let's examine time series at specific points of interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc02e7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define points of interest\n",
    "poi_locations = [\n",
    "    (scene_width//2, scene_height//2, \"Subsidence Center\"),\n",
    "    (scene_width//4, scene_height//4, \"Edge Region\"),\n",
    "    (3*scene_width//4, 3*scene_height//4, \"Reference Area\"),\n",
    "    (scene_width//6, 5*scene_height//6, \"Stable Region\")\n",
    "]\n",
    "\n",
    "# Extract time series for each POI\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = [ax1, ax2, ax3, ax4]\n",
    "colors = ['red', 'blue', 'green', 'orange']\n",
    "\n",
    "for i, (x, y, name) in enumerate(poi_locations):\n",
    "    # Extract time series\n",
    "    true_ts = true_deformation[y, x, :]\n",
    "    observed_ts_point = observed_ts[y, x, :]\n",
    "    atm_ts = total_atmosphere[y, x, :]\n",
    "    \n",
    "    # Plot on individual subplot\n",
    "    ax = axes[i]\n",
    "    ax.plot(dates, true_ts, 'k-', linewidth=2, label='True deformation')\n",
    "    ax.plot(dates, observed_ts_point, 'o-', color=colors[i], \n",
    "           alpha=0.7, markersize=4, label='Observed')\n",
    "    ax.plot(dates, atm_ts, '--', color='gray', alpha=0.7, label='Atmospheric')\n",
    "    \n",
    "    ax.set_title(f'{name}\\nLocation: ({x}, {y})')\n",
    "    ax.set_ylabel('Displacement (mm)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "    \n",
    "    # Format dates\n",
    "    if i >= 2:  # Bottom row\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "    else:\n",
    "        ax.set_xticklabels([])\n",
    "\n",
    "plt.suptitle('Time Series Examples at Different Locations', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate and display statistics for each POI\n",
    "print(\"Time Series Statistics:\")\n",
    "for x, y, name in poi_locations:\n",
    "    true_ts = true_deformation[y, x, :]\n",
    "    observed_ts_point = observed_ts[y, x, :]\n",
    "    \n",
    "    # Calculate linear velocity\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(time_years, true_ts)\n",
    "    \n",
    "    # Calculate RMSE between true and observed\n",
    "    rmse = np.sqrt(np.mean((true_ts - observed_ts_point)**2))\n",
    "    \n",
    "    print(f\"  {name}:\")\n",
    "    print(f\"    True velocity: {slope:.1f} ± {std_err:.1f} mm/year\")\n",
    "    print(f\"    RMSE (obs vs true): {rmse:.1f} mm\")\n",
    "    print(f\"    Total displacement: {true_ts[-1] - true_ts[0]:.1f} mm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f170af0b",
   "metadata": {},
   "source": [
    "## Time Series Decomposition\n",
    "\n",
    "Decompose the time series into trend, seasonal, and residual components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e77abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import savgol_filter\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def decompose_time_series(ts_data, time_years, window_length=25):\n",
    "    \"\"\"\n",
    "    Decompose time series into trend, seasonal, and residual components\n",
    "    \"\"\"\n",
    "    # 1. Extract linear trend\n",
    "    X = time_years.reshape(-1, 1)\n",
    "    reg = LinearRegression().fit(X, ts_data)\n",
    "    linear_trend = reg.predict(X)\n",
    "    \n",
    "    # 2. Extract nonlinear trend using Savitzky-Golay filter\n",
    "    if len(ts_data) > window_length:\n",
    "        smooth_trend = savgol_filter(ts_data, window_length, 3)\n",
    "    else:\n",
    "        smooth_trend = linear_trend\n",
    "    \n",
    "    # 3. Remove trend to get seasonal + residual\n",
    "    detrended = ts_data - smooth_trend\n",
    "    \n",
    "    # 4. Extract seasonal component (annual cycle)\n",
    "    # Fit sinusoidal model\n",
    "    def seasonal_model(t, a, b, c):\n",
    "        return a * np.sin(2 * np.pi * t) + b * np.cos(2 * np.pi * t) + c\n",
    "    \n",
    "    from scipy.optimize import curve_fit\n",
    "    try:\n",
    "        popt, _ = curve_fit(seasonal_model, time_years, detrended)\n",
    "        seasonal = seasonal_model(time_years, *popt)\n",
    "    except:\n",
    "        seasonal = np.zeros_like(time_years)\n",
    "    \n",
    "    # 5. Residual\n",
    "    residual = ts_data - smooth_trend - seasonal\n",
    "    \n",
    "    return {\n",
    "        'linear_trend': linear_trend,\n",
    "        'smooth_trend': smooth_trend,\n",
    "        'seasonal': seasonal,\n",
    "        'residual': residual,\n",
    "        'linear_velocity': reg.coef_[0]\n",
    "    }\n",
    "\n",
    "# Decompose time series for the subsidence center\n",
    "center_x, center_y = scene_width//2, scene_height//2\n",
    "center_ts = observed_ts[center_y, center_x, :]\n",
    "decomposition = decompose_time_series(center_ts, time_years)\n",
    "\n",
    "# Plot decomposition\n",
    "fig, axes = plt.subplots(4, 1, figsize=(14, 12))\n",
    "\n",
    "# Original time series\n",
    "axes[0].plot(dates, center_ts, 'b-o', markersize=4, label='Observed')\n",
    "axes[0].plot(dates, true_deformation[center_y, center_x, :], 'k--', \n",
    "            linewidth=2, label='True')\n",
    "axes[0].set_ylabel('Displacement (mm)')\n",
    "axes[0].set_title('Original Time Series (Subsidence Center)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Trend component\n",
    "axes[1].plot(dates, decomposition['smooth_trend'], 'r-', linewidth=2, label='Smooth trend')\n",
    "axes[1].plot(dates, decomposition['linear_trend'], 'r--', linewidth=2, label='Linear trend')\n",
    "axes[1].set_ylabel('Displacement (mm)')\n",
    "axes[1].set_title(f'Trend Component (Linear velocity: {decomposition[\"linear_velocity\"]:.1f} mm/year)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Seasonal component\n",
    "axes[2].plot(dates, decomposition['seasonal'], 'g-', linewidth=2)\n",
    "axes[2].set_ylabel('Displacement (mm)')\n",
    "axes[2].set_title('Seasonal Component')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "# Residual component\n",
    "axes[3].plot(dates, decomposition['residual'], 'orange', alpha=0.7)\n",
    "axes[3].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "axes[3].set_ylabel('Displacement (mm)')\n",
    "axes[3].set_xlabel('Date')\n",
    "axes[3].set_title(f'Residual Component (σ = {np.std(decomposition[\"residual\"]):.1f} mm)')\n",
    "axes[3].grid(True, alpha=0.3)\n",
    "\n",
    "# Format x-axis\n",
    "for ax in axes:\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Time Series Decomposition Results:\")\n",
    "print(f\"  Linear velocity: {decomposition['linear_velocity']:.2f} mm/year\")\n",
    "print(f\"  Seasonal amplitude: {np.std(decomposition['seasonal']):.1f} mm\")\n",
    "print(f\"  Residual std: {np.std(decomposition['residual']):.1f} mm\")\n",
    "print(f\"  Total variance explained: {1 - np.var(decomposition['residual'])/np.var(center_ts):.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d24e9e",
   "metadata": {},
   "source": [
    "## Spatial Pattern Analysis\n",
    "\n",
    "Analyze spatial patterns in the time series using Principal Component Analysis (PCA):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79608b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape data for PCA (pixels x time)\n",
    "ts_matrix = observed_ts.reshape(-1, n_acquisitions)\n",
    "\n",
    "# Remove mean (center the data)\n",
    "ts_matrix_centered = ts_matrix - np.mean(ts_matrix, axis=1, keepdims=True)\n",
    "\n",
    "# Apply PCA\n",
    "n_components = 5\n",
    "pca = PCA(n_components=n_components)\n",
    "pca_scores = pca.fit_transform(ts_matrix_centered.T)\n",
    "pca_components = pca.components_\n",
    "\n",
    "# Calculate explained variance\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "\n",
    "print(f\"PCA Analysis Results:\")\n",
    "for i in range(n_components):\n",
    "    print(f\"  Component {i+1}: {explained_variance[i]:.1%} variance\")\n",
    "print(f\"  Total explained variance (first {n_components} components): {cumulative_variance[-1]:.1%}\")\n",
    "\n",
    "# Visualize PCA results\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
    "\n",
    "# Plot explained variance\n",
    "axes[0, 0].bar(range(1, n_components+1), explained_variance, alpha=0.7)\n",
    "axes[0, 0].plot(range(1, n_components+1), cumulative_variance, 'ro-')\n",
    "axes[0, 0].set_xlabel('Component')\n",
    "axes[0, 0].set_ylabel('Explained Variance')\n",
    "axes[0, 0].set_title('PCA Explained Variance')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot temporal patterns (first 3 components)\n",
    "for i in range(3):\n",
    "    row, col = (0, 1) if i == 0 else (0, 2) if i == 1 else (1, 0)\n",
    "    \n",
    "    axes[row, col].plot(dates, pca_scores[:, i], 'o-', linewidth=2)\n",
    "    axes[row, col].set_title(f'PC{i+1} Temporal Pattern\\n({explained_variance[i]:.1%} variance)')\n",
    "    axes[row, col].set_ylabel('PC Score')\n",
    "    axes[row, col].grid(True, alpha=0.3)\n",
    "    axes[row, col].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot spatial patterns (first 3 components)\n",
    "for i in range(3):\n",
    "    row, col = (1, 1) if i == 0 else (1, 2) if i == 1 else (2, 0)\n",
    "    \n",
    "    spatial_pattern = pca_components[i, :].reshape(scene_height, scene_width)\n",
    "    im = axes[row, col].imshow(spatial_pattern, cmap='RdBu_r')\n",
    "    axes[row, col].set_title(f'PC{i+1} Spatial Pattern')\n",
    "    axes[row, col].set_xticks([])\n",
    "    axes[row, col].set_yticks([])\n",
    "    plt.colorbar(im, ax=axes[row, col], shrink=0.8)\n",
    "\n",
    "# Reconstruction quality\n",
    "reconstructed = np.dot(pca_scores[:, :3], pca_components[:3, :]).T\n",
    "reconstructed += np.mean(ts_matrix, axis=1, keepdims=True)\n",
    "\n",
    "# Show reconstruction for center pixel\n",
    "center_idx = center_y * scene_width + center_x\n",
    "axes[2, 1].plot(dates, ts_matrix[center_idx, :], 'b-o', label='Original', alpha=0.7)\n",
    "axes[2, 1].plot(dates, reconstructed[center_idx, :], 'r-', linewidth=2, label='PC1-3 reconstruction')\n",
    "axes[2, 1].set_title('PCA Reconstruction\\n(Center pixel)')\n",
    "axes[2, 1].set_ylabel('Displacement (mm)')\n",
    "axes[2, 1].legend()\n",
    "axes[2, 1].grid(True, alpha=0.3)\n",
    "axes[2, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Calculate reconstruction RMSE map\n",
    "rmse_map = np.sqrt(np.mean((ts_matrix - reconstructed)**2, axis=1))\n",
    "rmse_map = rmse_map.reshape(scene_height, scene_width)\n",
    "\n",
    "im = axes[2, 2].imshow(rmse_map, cmap='viridis')\n",
    "axes[2, 2].set_title('Reconstruction RMSE\\n(3 components)')\n",
    "axes[2, 2].set_xticks([])\n",
    "axes[2, 2].set_yticks([])\n",
    "plt.colorbar(im, ax=axes[2, 2], shrink=0.8, label='RMSE (mm)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nPCA Reconstruction Quality:\")\n",
    "print(f\"  Mean RMSE: {np.mean(rmse_map):.1f} mm\")\n",
    "print(f\"  Max RMSE: {np.max(rmse_map):.1f} mm\")\n",
    "print(f\"  Reconstruction efficiency: {1 - np.mean(rmse_map**2)/np.var(ts_matrix):.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053f8dbf",
   "metadata": {},
   "source": [
    "## Atmospheric Correction\n",
    "\n",
    "Demonstrate atmospheric phase correction techniques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a62c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def atmospheric_correction_pca(ts_data, n_components_remove=2):\n",
    "    \"\"\"\n",
    "    Remove atmospheric components using PCA\n",
    "    Assumes first few PC components contain atmospheric signals\n",
    "    \"\"\"\n",
    "    # Center the data\n",
    "    ts_centered = ts_data - np.mean(ts_data, axis=2, keepdims=True)\n",
    "    ts_matrix = ts_centered.reshape(-1, ts_centered.shape[2])\n",
    "    \n",
    "    # Apply PCA\n",
    "    pca = PCA()\n",
    "    pca_scores = pca.fit_transform(ts_matrix.T)\n",
    "    pca_components = pca.components_\n",
    "    \n",
    "    # Remove first n components (assumed to be atmospheric)\n",
    "    pca_scores_corrected = pca_scores.copy()\n",
    "    pca_scores_corrected[:, :n_components_remove] = 0\n",
    "    \n",
    "    # Reconstruct corrected time series\n",
    "    ts_corrected = np.dot(pca_scores_corrected, pca_components).T\n",
    "    ts_corrected += np.mean(ts_data, axis=2, keepdims=True)\n",
    "    \n",
    "    return ts_corrected.reshape(ts_data.shape)\n",
    "\n",
    "def atmospheric_correction_spatial_filter(ts_data, filter_size=10):\n",
    "    \"\"\"\n",
    "    Remove atmospheric effects using spatial high-pass filtering\n",
    "    \"\"\"\n",
    "    from scipy.ndimage import uniform_filter\n",
    "    \n",
    "    corrected_ts = np.zeros_like(ts_data)\n",
    "    \n",
    "    for t in range(ts_data.shape[2]):\n",
    "        # Apply spatial low-pass filter to estimate atmospheric component\n",
    "        atm_estimate = uniform_filter(ts_data[:, :, t], size=filter_size)\n",
    "        \n",
    "        # Subtract atmospheric estimate\n",
    "        corrected_ts[:, :, t] = ts_data[:, :, t] - atm_estimate\n",
    "    \n",
    "    return corrected_ts\n",
    "\n",
    "# Apply atmospheric corrections\n",
    "print(\"Applying atmospheric corrections...\")\n",
    "\n",
    "# PCA-based correction\n",
    "ts_corrected_pca = atmospheric_correction_pca(observed_ts, n_components_remove=2)\n",
    "\n",
    "# Spatial filter correction\n",
    "ts_corrected_spatial = atmospheric_correction_spatial_filter(observed_ts, filter_size=15)\n",
    "\n",
    "# Compare correction methods\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "\n",
    "# Time index for visualization\n",
    "t_idx = n_acquisitions // 2\n",
    "\n",
    "# Original data\n",
    "im1 = axes[0, 0].imshow(observed_ts[:, :, t_idx], cmap='RdBu_r')\n",
    "axes[0, 0].set_title('Original Observed')\n",
    "plt.colorbar(im1, ax=axes[0, 0], shrink=0.8)\n",
    "\n",
    "# PCA corrected\n",
    "im2 = axes[0, 1].imshow(ts_corrected_pca[:, :, t_idx], cmap='RdBu_r')\n",
    "axes[0, 1].set_title('PCA Corrected')\n",
    "plt.colorbar(im2, ax=axes[0, 1], shrink=0.8)\n",
    "\n",
    "# Spatial filter corrected\n",
    "im3 = axes[0, 2].imshow(ts_corrected_spatial[:, :, t_idx], cmap='RdBu_r')\n",
    "axes[0, 2].set_title('Spatial Filter Corrected')\n",
    "plt.colorbar(im3, ax=axes[0, 2], shrink=0.8)\n",
    "\n",
    "# True deformation (reference)\n",
    "im4 = axes[0, 3].imshow(true_deformation[:, :, t_idx], cmap='RdBu_r')\n",
    "axes[0, 3].set_title('True Deformation')\n",
    "plt.colorbar(im4, ax=axes[0, 3], shrink=0.8)\n",
    "\n",
    "# Time series comparison at center pixel\n",
    "center_original = observed_ts[center_y, center_x, :]\n",
    "center_pca = ts_corrected_pca[center_y, center_x, :]\n",
    "center_spatial = ts_corrected_spatial[center_y, center_x, :]\n",
    "center_true = true_deformation[center_y, center_x, :]\n",
    "\n",
    "axes[1, 0].plot(dates, center_original, 'b-o', alpha=0.7, label='Original')\n",
    "axes[1, 0].plot(dates, center_true, 'k--', linewidth=2, label='True')\n",
    "axes[1, 0].set_title('Original vs True')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "axes[1, 1].plot(dates, center_pca, 'r-o', alpha=0.7, label='PCA corrected')\n",
    "axes[1, 1].plot(dates, center_true, 'k--', linewidth=2, label='True')\n",
    "axes[1, 1].set_title('PCA Corrected vs True')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "axes[1, 2].plot(dates, center_spatial, 'g-o', alpha=0.7, label='Spatial corrected')\n",
    "axes[1, 2].plot(dates, center_true, 'k--', linewidth=2, label='True')\n",
    "axes[1, 2].set_title('Spatial Corrected vs True')\n",
    "axes[1, 2].legend()\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "axes[1, 2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# RMSE comparison\n",
    "rmse_original = np.sqrt(np.mean((observed_ts - true_deformation)**2))\n",
    "rmse_pca = np.sqrt(np.mean((ts_corrected_pca - true_deformation)**2))\n",
    "rmse_spatial = np.sqrt(np.mean((ts_corrected_spatial - true_deformation)**2))\n",
    "\n",
    "methods = ['Original', 'PCA Corrected', 'Spatial Corrected']\n",
    "rmse_values = [rmse_original, rmse_pca, rmse_spatial]\n",
    "\n",
    "bars = axes[1, 3].bar(methods, rmse_values, color=['blue', 'red', 'green'], alpha=0.7)\n",
    "axes[1, 3].set_ylabel('RMSE (mm)')\n",
    "axes[1, 3].set_title('Correction Performance')\n",
    "axes[1, 3].grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars, rmse_values):\n",
    "    axes[1, 3].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
    "                   f'{value:.1f}', ha='center', va='bottom')\n",
    "\n",
    "# Remove ticks for image plots\n",
    "for i in range(4):\n",
    "    axes[0, i].set_xticks([])\n",
    "    axes[0, i].set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Atmospheric Correction Results:\")\n",
    "print(f\"  Original RMSE: {rmse_original:.1f} mm\")\n",
    "print(f\"  PCA corrected RMSE: {rmse_pca:.1f} mm ({(rmse_original-rmse_pca)/rmse_original*100:+.1f}%)\")\n",
    "print(f\"  Spatial corrected RMSE: {rmse_spatial:.1f} mm ({(rmse_original-rmse_spatial)/rmse_original*100:+.1f}%)\")\n",
    "\n",
    "# Best performing method\n",
    "best_method_idx = np.argmin(rmse_values[1:])\n",
    "best_method = methods[best_method_idx + 1]\n",
    "print(f\"  Best performing method: {best_method}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d2d862",
   "metadata": {},
   "source": [
    "## Velocity Estimation and Uncertainty\n",
    "\n",
    "Estimate linear velocities and their uncertainties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f98710f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_velocity_map(ts_data, time_years, method='linear'):\n",
    "    \"\"\"\n",
    "    Estimate velocity map from time series data\n",
    "    \"\"\"\n",
    "    height, width, n_times = ts_data.shape\n",
    "    \n",
    "    velocity_map = np.zeros((height, width))\n",
    "    velocity_std_map = np.zeros((height, width))\n",
    "    r_squared_map = np.zeros((height, width))\n",
    "    \n",
    "    X = time_years.reshape(-1, 1)\n",
    "    \n",
    "    for i in range(height):\n",
    "        for j in range(width):\n",
    "            ts = ts_data[i, j, :]\n",
    "            \n",
    "            # Linear regression\n",
    "            slope, intercept, r_value, p_value, std_err = stats.linregress(time_years, ts)\n",
    "            \n",
    "            velocity_map[i, j] = slope\n",
    "            velocity_std_map[i, j] = std_err\n",
    "            r_squared_map[i, j] = r_value**2\n",
    "    \n",
    "    return velocity_map, velocity_std_map, r_squared_map\n",
    "\n",
    "# Estimate velocities for different datasets\n",
    "print(\"Estimating velocity maps...\")\n",
    "\n",
    "# True velocities\n",
    "vel_true, vel_std_true, r2_true = estimate_velocity_map(true_deformation, time_years)\n",
    "\n",
    "# Observed velocities (with atmospheric effects)\n",
    "vel_observed, vel_std_observed, r2_observed = estimate_velocity_map(observed_ts, time_years)\n",
    "\n",
    "# Corrected velocities (using best correction method)\n",
    "if rmse_pca < rmse_spatial:\n",
    "    vel_corrected, vel_std_corrected, r2_corrected = estimate_velocity_map(ts_corrected_pca, time_years)\n",
    "    correction_label = \"PCA Corrected\"\n",
    "else:\n",
    "    vel_corrected, vel_std_corrected, r2_corrected = estimate_velocity_map(ts_corrected_spatial, time_years)\n",
    "    correction_label = \"Spatial Corrected\"\n",
    "\n",
    "# Visualize velocity maps\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "\n",
    "# Velocity maps\n",
    "vmin, vmax = np.percentile(vel_true, [5, 95])\n",
    "\n",
    "im1 = axes[0, 0].imshow(vel_true, cmap='RdBu_r', vmin=vmin, vmax=vmax)\n",
    "axes[0, 0].set_title('True Velocity')\n",
    "plt.colorbar(im1, ax=axes[0, 0], shrink=0.8, label='mm/year')\n",
    "\n",
    "im2 = axes[0, 1].imshow(vel_observed, cmap='RdBu_r', vmin=vmin, vmax=vmax)\n",
    "axes[0, 1].set_title('Observed Velocity')\n",
    "plt.colorbar(im2, ax=axes[0, 1], shrink=0.8, label='mm/year')\n",
    "\n",
    "im3 = axes[0, 2].imshow(vel_corrected, cmap='RdBu_r', vmin=vmin, vmax=vmax)\n",
    "axes[0, 2].set_title(f'{correction_label} Velocity')\n",
    "plt.colorbar(im3, ax=axes[0, 2], shrink=0.8, label='mm/year')\n",
    "\n",
    "# Velocity difference\n",
    "vel_diff = vel_corrected - vel_true\n",
    "im4 = axes[0, 3].imshow(vel_diff, cmap='RdBu_r')\n",
    "axes[0, 3].set_title('Velocity Difference\\n(Corrected - True)')\n",
    "plt.colorbar(im4, ax=axes[0, 3], shrink=0.8, label='mm/year')\n",
    "\n",
    "# Uncertainty maps\n",
    "im5 = axes[1, 0].imshow(vel_std_true, cmap='viridis')\n",
    "axes[1, 0].set_title('True Velocity Uncertainty')\n",
    "plt.colorbar(im5, ax=axes[1, 0], shrink=0.8, label='mm/year')\n",
    "\n",
    "im6 = axes[1, 1].imshow(vel_std_observed, cmap='viridis')\n",
    "axes[1, 1].set_title('Observed Velocity Uncertainty')\n",
    "plt.colorbar(im6, ax=axes[1, 1], shrink=0.8, label='mm/year')\n",
    "\n",
    "im7 = axes[1, 2].imshow(r2_corrected, cmap='viridis', vmin=0, vmax=1)\n",
    "axes[1, 2].set_title('R² (Goodness of Fit)')\n",
    "plt.colorbar(im7, ax=axes[1, 2], shrink=0.8, label='R²')\n",
    "\n",
    "# Scatter plot: True vs Corrected velocities\n",
    "# Sample for plotting\n",
    "sample_indices = np.random.choice(n_pixels, 1000, replace=False)\n",
    "vel_true_flat = vel_true.flatten()[sample_indices]\n",
    "vel_corrected_flat = vel_corrected.flatten()[sample_indices]\n",
    "\n",
    "axes[1, 3].scatter(vel_true_flat, vel_corrected_flat, alpha=0.6, s=20)\n",
    "axes[1, 3].plot([vmin, vmax], [vmin, vmax], 'r--', linewidth=2, label='1:1 line')\n",
    "axes[1, 3].set_xlabel('True Velocity (mm/year)')\n",
    "axes[1, 3].set_ylabel('Corrected Velocity (mm/year)')\n",
    "axes[1, 3].set_title('Velocity Comparison')\n",
    "axes[1, 3].legend()\n",
    "axes[1, 3].grid(True, alpha=0.3)\n",
    "\n",
    "# Calculate correlation\n",
    "correlation = np.corrcoef(vel_true.flatten(), vel_corrected.flatten())[0, 1]\n",
    "axes[1, 3].text(0.05, 0.95, f'Correlation: {correlation:.3f}', \n",
    "               transform=axes[1, 3].transAxes, \n",
    "               bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "# Remove ticks for image plots\n",
    "for i in range(3):\n",
    "    for j in range(4):\n",
    "        if j < 3:  # Skip scatter plot\n",
    "            axes[i, j].set_xticks([])\n",
    "            axes[i, j].set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate velocity statistics\n",
    "vel_rmse = np.sqrt(np.mean((vel_corrected - vel_true)**2))\n",
    "vel_bias = np.mean(vel_corrected - vel_true)\n",
    "vel_std_diff = np.std(vel_corrected - vel_true)\n",
    "\n",
    "print(f\"\\nVelocity Estimation Results:\")\n",
    "print(f\"  True velocity range: {np.min(vel_true):.1f} to {np.max(vel_true):.1f} mm/year\")\n",
    "print(f\"  Velocity RMSE: {vel_rmse:.2f} mm/year\")\n",
    "print(f\"  Velocity bias: {vel_bias:.2f} mm/year\")\n",
    "print(f\"  Velocity correlation: {correlation:.3f}\")\n",
    "print(f\"  Mean R²: {np.mean(r2_corrected):.3f}\")\n",
    "print(f\"  Pixels with R² > 0.8: {np.sum(r2_corrected > 0.8)/n_pixels*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e07172",
   "metadata": {},
   "source": [
    "## Summary and Best Practices\n",
    "\n",
    "This notebook demonstrated comprehensive time series analysis techniques for InSAR data:\n",
    "\n",
    "### Key Techniques Covered:\n",
    "1. **Time Series Decomposition**: Separating trend, seasonal, and residual components\n",
    "2. **Spatial Pattern Analysis**: Using PCA to identify common spatial patterns\n",
    "3. **Atmospheric Correction**: PCA-based and spatial filtering approaches\n",
    "4. **Velocity Estimation**: Linear trend estimation with uncertainty quantification\n",
    "5. **Quality Assessment**: R² and correlation analysis\n",
    "\n",
    "### Best Practices for Time Series InSAR:\n",
    "\n",
    "1. **Data Quality**:\n",
    "   - Use consistent acquisition geometry\n",
    "   - Maintain regular temporal sampling\n",
    "   - Consider coherence thresholds\n",
    "\n",
    "2. **Atmospheric Correction**:\n",
    "   - Apply spatial filtering for stratified delays\n",
    "   - Use PCA for common mode removal\n",
    "   - Consider external weather data\n",
    "\n",
    "3. **Trend Analysis**:\n",
    "   - Account for nonlinear deformation\n",
    "   - Consider seasonal variations\n",
    "   - Validate with independent data\n",
    "\n",
    "4. **Uncertainty Quantification**:\n",
    "   - Estimate velocity uncertainties\n",
    "   - Consider temporal correlation\n",
    "   - Assess model goodness of fit\n",
    "\n",
    "### Performance Summary:\n",
    "- Atmospheric effects were successfully reduced by **{:.1f}%** using correction techniques\n",
    "- Velocity estimation achieved **{:.1f} mm/year** RMSE\n",
    "- **{:.1f}%** of pixels showed good linear fit (R² > 0.8)\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Explore **Advanced Workflows** in `05_advanced_workflows.ipynb`\n",
    "- Learn about **Multi-temporal InSAR** techniques\n",
    "- Study **Persistent Scatterer** methods\n",
    "- Investigate **Machine Learning** applications\n",
    "\n",
    "---\n",
    "\n",
    "*For more advanced time series techniques and applications, consult the SARPYX documentation and recent scientific literature.*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
