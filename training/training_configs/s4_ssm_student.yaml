# Configuration for S4 SSM Student Model (Knowledge Distillation)
# This is the target student model without selectivity mechanism

# Data directory
data_dir: "/Data/sar_focusing"

# Student Model configuration (customizable for different compression levels)
model:
  name: "s4_ssm"
  dim_head: 1
  input_dim: 2
  model_dim: 4      # Can be reduced further: 32, 48 for more compression
  state_dim: 16     # Can be reduced: 256, 384 for faster inference  
  output_dim: 1
  num_layers: 4      # Can be reduced: 4, 5 for smaller model
  dropout: 0.2
  use_pos_encoding: true
  complex_valued: true
  use_selectivity: false  # Key difference: no selectivity mechanism
  activation_function: 'leakyrelu'
# Alternative smaller configurations (uncomment to use):
# Small student (50% compression):
#   model_dim: 32
#   state_dim: 256  
#   num_layers: 4
#
# Tiny student (75% compression):
#   model_dim: 24
#   state_dim: 128
#   num_layers: 3

# Training configuration for knowledge distillation
training:
  patience: 40
  delta: 0.00001
  weight_decay: 1e-3
  num_epochs: 300
  lr: 5e-4  # Lower learning rate for distillation
  scheduler_type: "cosine"
  loss_fn: "complex_mse"
  save_dir: "./results/s4_ssm_distilled"
  mode: "parallel"

# Knowledge Distillation specific parameters
distillation:
  temperature: 4.0
  alpha: 0.3        # Weight for student loss (ground truth)
  beta: 0.5         # Weight for distillation loss (teacher knowledge)
  gamma: 0.2        # Weight for feature matching loss
  feature_matching: true
  freeze_teacher: true

# Device configuration
device: "cuda"

# Dataloader configuration (same as teacher)
dataloader:
  level_from: "rc"
  level_to: "az"
  num_workers: 0
  patch_mode: "rectangular"
  patch_size: [1000, 1]
  buffer: [1000, 1000]
  stride: [1000, 1]
  max_base_sample_size: [5000, 5000]
  shuffle_files: false
  complex_valued: true
  save_samples: false
  backend: "zarr"
  verbose: false
  cache_size: 1000
  online: true
  concatenate_patches: false
  concat_axis: 0
  positional_encoding: true
  
  train: 
    batch_size: 32
    samples_per_prod: 10000
    patch_order: "row"
    max_products: 10
    filters: {
      "years": [2023],
      "polarizations": ["vv"]
    }
    
  validation: 
    batch_size: 32
    samples_per_prod: 1000
    patch_order: "row"
    max_products: 10
    filters: {
      "years": [2024],
      "polarizations": ["vv"]
    }
    
  test: 
    batch_size: 32
    samples_per_prod: 5000
    patch_order: "row"
    max_products: 10
    filters: {
      "years": [2025],
      "polarizations": ["vv"]
    }
    
  inference: 
    batch_size: 32
    samples_per_prod: 5000
    patch_order: "row"
    max_products: 1
    filters: {
      "years": [2023],
      "polarizations": ["vv"]
    }

# Transform configuration (same as teacher)
transforms:
  normalize: true
  complex_valued: true
  rc_min: -2000
  rc_max: 2000
  gt_min: -2000
  gt_max: 2000