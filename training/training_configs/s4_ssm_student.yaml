# Configuration for S4 SSM Student Model (Knowledge Distillation)
# This is the target student model without selectivity mechanism

# Data directory
data_dir: "/Data/sar_focusing"

# Student Model configuration (customizable for different compression levels)
model:
  name: "s4_ssm_final"
  input_dim: 3      
  model_dim: 2      
  state_dim: 8      
  output_dim: 2    
  num_layers: 4      
  activation_function: 'leakyrelu'


# Training configuration for knowledge distillation
training:
  patience: 40
  delta: 0.00001
  weight_decay: 0.0001
  num_epochs: 300
  lr: 0.0005  # Lower learning rate for distillation
  scheduler_type: "cosine"
  loss_fn: "complex_mse"
  save_dir: "./results/s4_ssm_distilled"
  mode: "parallel"
  wraper: false

# Knowledge Distillation specific parameters
distillation:
  temperature: 4.0
  alpha: 0.3        # Weight for student loss (ground truth)
  beta: 0.5         # Weight for distillation loss (teacher knowledge)
  gamma: 0.2        # Weight for feature matching loss
  feature_matching: true
  freeze_teacher: true

# Device configuration
device: "cuda"

# Dataloader configuration (same as teacher)
dataloader:
  level_from: "rc"
  level_to: "az"
  num_workers: 0
  patch_mode: "rectangular"
  patch_size: [1000, 1]
  buffer: [1000, 1000]
  stride: [1000, 1]
  max_base_sample_size: [50000, 50000]
  shuffle_files: false
  complex_valued: true
  save_samples: false
  backend: "zarr"
  verbose: false
  cache_size: 1000
  online: true
  concatenate_patches: false
  concat_axis: 0
  positional_encoding: true
  
  train: 
    batch_size: 32
    samples_per_prod: 10000
    patch_order: "row"
    max_products: 10
    filters: {
      "years": [2023],
      "polarizations": ["vv"]
    }
    
  validation: 
    batch_size: 32
    samples_per_prod: 1000
    patch_order: "row"
    max_products: 1
    filters: {
      "years": [2024],
      "polarizations": ["vv"]
    }
    
  test: 
    batch_size: 32
    samples_per_prod: 5000
    patch_order: "row"
    max_products: 10
    filters: {
      "years": [2025],
      "polarizations": ["vv"]
    }
    
  inference: 
    batch_size: 32
    samples_per_prod: 5000
    patch_order: "row"
    max_products: 1
    filters: {
      "years": [2023],
      "polarizations": ["vv"]
    }

# Transform configuration (same as teacher)
transforms:
  normalize: true
  complex_valued: true
  rc_min: -2000
  rc_max: 2000
  gt_min: -2000
  gt_max: 2000