# Example Configuration with Parameter Sweeps
# This file demonstrates how to set up parameter sweeps for hyperparameter optimization

# Data directory
data_dir: "/Data/sar_focusing"

# Model configuration
model:
  name: "s4_ssm"
  dim_head: 1
  input_dim: 2
  model_dim: 2
  state_dim: 512
  output_dim: 1
  num_layers: 4
  dropout: 0.05  # Reduced from 0.1 to prevent amplitude suppression
  use_pos_encoding: true
  complex_valued: true
  use_selectivity: false
  activation_function: 'identity'  # DISABLED activation to preserve amplitude range!
                                    # ReLU was clipping negatives â†’ destroying dynamic range
                                    # With 4 layers, this compounds catastrophically
                                    # Options: 'identity' (RECOMMENDED for SAR), 'tanh', 'gelu', 'relu' (AVOID!)
  # S4D Initialization Strategy (IMPORTANT for amplitude matching)
  scaling: 'log'  # Options: 'inverse' (default), 'linear', 'log' (RECOMMENDED), 'sqrt', 'random'
  
  # NEW: Amplitude preservation parameters (CRITICAL FIX for output scaling)
  use_skip_connection: true  # Skip connection from input to output (preserves amplitude)
  
  # pretrained_path: "./results/s4_ssm_smaller_cols/last.ckpt"
  # start_key: "model." # Key prefix to load weights from checkpoint

# Training configuration
training:
  patience: 30
  delta: 0.00001
  weight_decay: 0.0001   # Reduced weight decay to prevent over-regularization
  epochs: 100            # Increased epochs for better convergence
  lr: 0.0005             # Increased learning rate for better exploration
  scheduler_type: "cosine"

  save_dir: "./results/s4_ssm"
  mode: "parallel"
  warmup_epochs: 3
  warmup_start_lr: 0.00001
  
  # Enhanced training options for PyTorch Lightning
  gradient_clip_val: 0.5  # Stricter gradient clipping to prevent instability
  accumulate_grad_batches: 2  # Effective batch size of 128 (64*2)
  #precision: "16-mixed"  # Mixed precision training for faster training and less memory
  
  loss_fn: "sar_focusing"  # Use comprehensive SAR focusing loss (IMPROVED VERSION)
  
  # SAR Focusing Loss Weights (OPTIMIZED with distribution-aware MSE + KDE)
  lambda_rec: 0.3          # Reconstruction loss weight (baseline)
  lambda_focus: 0.3       # Focus quality weight (disabled - not critical for 1D focusing)
  lambda_dist: 0.4         # Distribution matching weight (KDE-based, requires batch_size >= 10)
  lambda_tv: 0.0           # Total variation weight (disabled for now)
  
  # SAR Focusing Loss Configuration (USING NEW KDE-BASED DISTRIBUTION MATCHING)
  rec_loss_type: "complex_mse"  # Will be swept: complex_mse, huber, complex_mae, polarimetric
                                # Options: 'mse', 'mae', 'complex_mse', 'complex_mae', 
                                #          'distribution_aware_mse', 'huber', 'polarimetric'
  focus_metric: "structure"     # Focus quality metric: 'contrast', 'variance', 'entropy', 'structure' (SSIM)
  dist_metric: "histogram"      # Distribution matching using KDE (Kernel Density Estimation)
                                # NEW: Uses Gaussian KDE instead of raw histograms
                                # Provides smooth probability density estimates
                                # IMPORTANT: Requires batch_size >= 10 for reliable KDE
                                # Best with batch_size >= 20 for near-Gaussian distributions
                                # Options: 'moments', 'histogram' (KDE), 'mmd', 'none'
  use_tv: false                 # Disable total variation for now
  use_adaptive_weights: false   # Disable adaptive weighting (manual control for debugging)

  # Additional stability measures
  gradient_clip_algorithm: "norm" 
  check_val_every_n_epoch: 1     
  
  # WandB configuration
  wandb_project: "ssm4sar"
  wandb_entity: "gabrieledaga01-universit-di-bologna"  # Updated to your entity
  wandb_tags: ["sweep", "hyperparameter_optimization", "s4_ssm"]

# Device configuration
device: "cuda"


# Dataloader configuration
dataloader:
  data_dir: "/Data/sar_focusing"
  level_from: "rc"
  level_to: "az"
  num_workers: 0
  patch_mode: "rectangular"
  patch_size: [1000, 1]
  buffer: [1000, 1000]
  stride: [1000, 1]
  max_base_sample_size: [50000, 50000]
  shuffle_files: true
  concatenate_patches: false
  complex_valued: true
  save_samples: false
  online: true
  verbose: false
  use_balanced_sampling: true
  
  # Split configurations
  train:
    batch_size: 30
    samples_per_prod: 3000
    patch_order: "row"
    max_products: 8
    filters: {
      "polarizations": ["vv"],
      "parts": ["PT1", "PT2", "PT4"]
    }

  
  validation:
    batch_size: 30
    samples_per_prod: 3000
    patch_order: "row"
    max_products: 5
    filters: {
      "polarizations": [ "vv"],
      "parts": ["PT1", "PT2", "PT4"]
    }
  test:
    batch_size: 30
    samples_per_prod: 3000
    patch_order: "row"
    max_products: 5
    filters: {
      "polarizations": ["vv"],
      "parts": ["PT1", "PT2", "PT4"]
    }

  inference:
    batch_size: 25
    samples_per_prod: 10000
    patch_order: "row"
    max_products: 5
    filters: {
      "polarizations": ["vv"],
      "parts": ["PT1", "PT2", "PT4"]
    }

# Transform configuration
transforms:
  normalize: true
  complex_valued: true
  normalization_type: "zscore"  # Standardization (z-score normalization)
  rc_mean: 0
  rc_std: 500
  gt_mean: 0
  gt_std: 500

# Sweep parameters - sweep over reconstruction loss types
_sweep_params:
  training.rec_loss_type:
    values: ["complex_mse", "huber", "complex_mae", "polarimetric"]
