# Configuration for S4 SSM Student Model (Knowledge Distillation)
# This is the target student model without selectivity mechanism

# Data directory
data_dir: "/home/zeshan/projects/sarSSM/data/maya4_data/validation"

# Student Model configuration (customizable for different compression levels)
model:
  name: "s4_ssm_recurrent_minimal"

# TODO: do I still need these parameters?
# Training configuration for knowledge distillation
training:
  patience: 40
  delta: 0.00001
  weight_decay: 0.0001
  num_epochs: 300
  lr: 0.0005  # Lower learning rate for distillation
  scheduler_type: "cosine"
  loss_fn: "complex_mse"
  save_dir: "./results/s4_ssm_qat"
  mode: "parallel"
  wraper: false

  # Knowledge Distillation specific parameters TODO: still need?
distillation:
  temperature: 4.0
  alpha: 0.3        # Weight for student loss (ground truth)
  beta: 0.5         # Weight for distillation loss (teacher knowledge)
  gamma: 0.2        # Weight for feature matching loss
  feature_matching: true
  freeze_teacher: true

# Device configuration
device: "cuda"

# Dataloader configuration (same as teacher)
dataloader:
  level_from: "rc"
  level_to: "az"
  num_workers: 0
  patch_mode: "rectangular"
  patch_size: [1000, 1]
  buffer: [1000, 1000]
  stride: [1000, 1]
  max_base_sample_size: [50000, 50000]
  shuffle_files: false
  complex_valued: true
  save_samples: false
  backend: "zarr"
  verbose: false
  cache_size: 1000
  online: false
  concatenate_patches: false
  concat_axis: 0
  positional_encoding: true
  
  train: 
    batch_size: 32
    samples_per_prod: 90000
    patch_order: "row"
    max_products: 10
    filters: {
      "years": [2023],
      "polarizations": ["hh"]
    }
    
  validation: 
    batch_size: 32
    samples_per_prod: 90000
    patch_order: "row"
    max_products: 1
    filters: {
      "years": [2024],
      "polarizations": ["hh"]
    }
    
  test: 
    batch_size: 32
    samples_per_prod: 90000
    patch_order: "row"
    max_products: 10
    filters: {
      "years": [2024],
      "polarizations": ["hh"]
    }
    
  inference: 
    batch_size: 32
    samples_per_prod: 90000
    patch_order: "row"
    max_products: 1
    filters: {
      "years": [2024],
      "polarizations": ["hh"]
    }

# Transform configuration (same as teacher)
transforms:
  normalize: true
  complex_valued: true
  rc_min: -2000
  rc_max: 2000
  gt_min: -2000
  gt_max: 2000
