{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d17ccc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import os \n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "import yaml\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import logging\n",
    "import time\n",
    "from typing import Dict, Any\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from model.model_utils import get_model_from_configs, create_model_with_pretrained\n",
    "from training.training_loops import get_training_loop_by_model_name\n",
    "from training.visualize import get_full_image_and_prediction, compute_metrics\n",
    "from sarpyx.utils.losses import get_loss_function\n",
    "from training_script import load_config\n",
    "from inference_script import create_test_dataloader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def setup_logging():\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(\"visualization.log\"),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    return logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b07f840",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_intensity_histograms(orig_gt, orig_pred, gt, pred, figsize=(20, 12), bins=100):\n",
    "    \"\"\"\n",
    "    Plot histograms of pixel intensities for original and processed data.\n",
    "    Separate plots for real and imaginary components.\n",
    "    \"\"\"\n",
    "    # Convert to numpy if needed\n",
    "    if hasattr(orig_gt, 'numpy'):\n",
    "        orig_gt = orig_gt.cpu().numpy()\n",
    "    if hasattr(orig_pred, 'numpy'):\n",
    "        orig_pred = orig_pred.cpu().numpy()\n",
    "    if hasattr(gt, 'numpy'):\n",
    "        gt = gt.cpu().numpy()\n",
    "    if hasattr(pred, 'numpy'):\n",
    "        pred = pred.cpu().numpy()\n",
    "    \n",
    "    # Create subplots: 2 rows (original vs processed) x 2 cols (real vs imaginary)\n",
    "    fig, axes = plt.subplots(2, 2, figsize=figsize)\n",
    "    \n",
    "    real_data_orig = np.concatenate([\n",
    "        orig_gt.real.flatten(), orig_pred.real.flatten()\n",
    "    ])\n",
    "    real_data_denorm = np.concatenate([\n",
    "        gt.real.flatten(), pred.real.flatten()\n",
    "    ])\n",
    "    imag_data_orig = np.concatenate([\n",
    "        orig_gt.imag.flatten(), orig_pred.imag.flatten()\n",
    "    ])\n",
    "    imag_data_denorm = np.concatenate([\n",
    "        gt.imag.flatten(), pred.imag.flatten()\n",
    "    ])\n",
    "    \n",
    "    real_range_orig = (np.min(real_data_orig), np.max(real_data_orig))\n",
    "    imag_range_orig = (np.min(imag_data_orig), np.max(imag_data_orig))\n",
    "    real_range_denorm = (np.min(real_data_denorm), np.max(real_data_denorm))\n",
    "    imag_range_denorm = (np.min(imag_data_denorm), np.max(imag_data_denorm))\n",
    "    \n",
    "    # Plot 1: Original data histograms - Real component\n",
    "    axes[0, 0].hist(orig_gt.real.flatten(), bins=bins, alpha=0.7, label='Original GT (Real)', \n",
    "                    color='blue', density=True, range=real_range_orig)\n",
    "    axes[0, 0].hist(orig_pred.real.flatten(), bins=bins, alpha=0.7, label='Original Pred (Real)', \n",
    "                    color='red', density=True, range=real_range_orig)\n",
    "    axes[0, 0].set_title('Original Data - Real Component')\n",
    "    axes[0, 0].set_xlabel('Pixel Intensity')\n",
    "    axes[0, 0].set_ylabel('Density')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    axes[0, 0].set_xlim(real_range_orig)\n",
    "    \n",
    "    # Plot 2: Original data histograms - Imaginary component\n",
    "    axes[0, 1].hist(orig_gt.imag.flatten(), bins=bins, alpha=0.7, label='Original GT (Imag)', \n",
    "                    color='blue', density=True, range=imag_range_orig)\n",
    "    axes[0, 1].hist(orig_pred.imag.flatten(), bins=bins, alpha=0.7, label='Original Pred (Imag)', \n",
    "                    color='red', density=True, range=imag_range_orig)\n",
    "    axes[0, 1].set_title('Original Data - Imaginary Component')\n",
    "    axes[0, 1].set_xlabel('Pixel Intensity')\n",
    "    axes[0, 1].set_ylabel('Density')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    axes[0, 1].set_xlim(imag_range_orig)\n",
    "      # Explicitly set x-axis limits\n",
    "\n",
    "    # Plot 3: Processed data histograms - Real component\n",
    "    axes[1, 0].hist(gt.real.flatten(), bins=bins, alpha=0.7, label='Processed GT (Real)', \n",
    "                    color='green', density=True, range=real_range_denorm)\n",
    "    axes[1, 0].hist(pred.real.flatten(), bins=bins, alpha=0.7, label='Processed Pred (Real)', \n",
    "                    color='orange', density=True, range=real_range_denorm)\n",
    "    axes[1, 0].set_title('Processed Data - Real Component')\n",
    "    axes[1, 0].set_xlabel('Pixel Intensity')\n",
    "    axes[1, 0].set_ylabel('Density')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    axes[1, 0].set_xlim(real_range_denorm)\n",
    "    \n",
    "    # Plot 4: Processed data histograms - Imaginary component\n",
    "    axes[1, 1].hist(gt.imag.flatten(), bins=bins, alpha=0.7, label='Processed GT (Imag)', \n",
    "                    color='green', density=True, range=imag_range_denorm)\n",
    "    axes[1, 1].hist(pred.imag.flatten(), bins=bins, alpha=0.7, label='Processed Pred (Imag)', \n",
    "                    color='orange', density=True, range=imag_range_denorm)\n",
    "    axes[1, 1].set_title('Processed Data - Imaginary Component')\n",
    "    axes[1, 1].set_xlabel('Pixel Intensity')\n",
    "    axes[1, 1].set_ylabel('Density')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    axes[1, 1].set_xlim(imag_range_denorm)\n",
    "    \n",
    "    # Add statistics text\n",
    "    for i, (data_gt, data_pred, title_suffix) in enumerate([\n",
    "        (orig_gt, orig_pred, \"Original\"),\n",
    "        (gt, pred, \"Processed\")\n",
    "    ]):\n",
    "        for j, component in enumerate(['real', 'imag']):\n",
    "            if component == 'real':\n",
    "                gt_comp, pred_comp = data_gt.real, data_pred.real\n",
    "            else:\n",
    "                gt_comp, pred_comp = data_gt.imag, data_pred.imag\n",
    "            \n",
    "            # Calculate statistics\n",
    "            gt_mean, gt_std = np.mean(gt_comp), np.std(gt_comp)\n",
    "            pred_mean, pred_std = np.mean(pred_comp), np.std(pred_comp)\n",
    "            \n",
    "            # Add stats text box\n",
    "            stats_text = f'GT: Î¼={gt_mean:.2f}, Ïƒ={gt_std:.2f}\\nPred: Î¼={pred_mean:.2f}, Ïƒ={pred_std:.2f}'\n",
    "            axes[i, j].text(0.02, 0.98, stats_text, transform=axes[i, j].transAxes, \n",
    "                           verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed statistics\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DETAILED STATISTICS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for data_name, data_gt, data_pred in [(\"Original\", orig_gt, orig_pred), (\"Processed\", gt, pred)]:\n",
    "        print(f\"\\n{data_name} Data:\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        for comp_name, gt_comp, pred_comp in [(\"Real\", data_gt.real, data_pred.real), \n",
    "                                              (\"Imaginary\", data_gt.imag, data_pred.imag)]:\n",
    "            print(f\"\\n{comp_name} Component:\")\n",
    "            print(f\"  GT    - Mean: {np.mean(gt_comp):8.4f}, Std: {np.std(gt_comp):8.4f}, \"\n",
    "                  f\"Min: {np.min(gt_comp):8.4f}, Max: {np.max(gt_comp):8.4f}\")\n",
    "            print(f\"  Pred  - Mean: {np.mean(pred_comp):8.4f}, Std: {np.std(pred_comp):8.4f}, \"\n",
    "                  f\"Min: {np.min(pred_comp):8.4f}, Max: {np.max(pred_comp):8.4f}\")\n",
    "            print(f\"  Diff  - Mean: {np.mean(gt_comp - pred_comp):8.4f}, \"\n",
    "                  f\"Std: {np.std(gt_comp - pred_comp):8.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45aefa7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-09 08:00:50,193 - INFO - Configuration Summary:\n",
      "2025-10-09 08:00:50,194 - INFO -   Data directory: /Data/sar_focusing\n",
      "2025-10-09 08:00:50,195 - INFO -   Level from: rc\n",
      "2025-10-09 08:00:50,195 - INFO -   Level to: az\n",
      "2025-10-09 08:00:50,196 - INFO -   Patch size: [10000, 1]\n",
      "2025-10-09 08:00:50,196 - INFO -   Batch size: 32\n",
      "2025-10-09 08:00:50,197 - INFO -   Save directory: ./visualizations\n",
      "2025-10-09 08:00:50,198 - INFO - Creating test dataloader...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test config: {'batch_size': 32, 'samples_per_prod': 5000, 'patch_order': 'row', 'max_products': 1, 'filters': {'years': [2023], 'polarizations': ['vv']}}\n",
      "Creating dataloader with config: {'data_dir': '/Data/sar_focusing', 'level_from': 'rc', 'level_to': 'az', 'num_workers': 0, 'patch_mode': 'rectangular', 'patch_size': (10000, 1), 'buffer': (1000, 1000), 'stride': (10000, 1), 'max_base_sample_size': [50000, 50000], 'shuffle_files': False, 'complex_valued': True, 'save_samples': False, 'backend': 'zarr', 'verbose': False, 'cache_size': 1000, 'online': True, 'concatenate_patches': False, 'concat_axis': 0, 'positional_encoding': True, 'transform': SARTransform(\n",
      "  (transform_raw): ComplexNormalizationModule()\n",
      "  (transform_rc): ComplexNormalizationModule()\n",
      "  (transform_rcmc): ComplexNormalizationModule()\n",
      "  (transform_az): ComplexNormalizationModule()\n",
      "), 'block_pattern': None, 'batch_size': 32, 'samples_per_prod': 5000, 'patch_order': 'row', 'max_products': 1}\n",
      "Total files found in remote repository: 967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-09 08:00:53,121 - INFO - Created test dataloader with 157 batches\n",
      "2025-10-09 08:00:53,122 - INFO - Dataset contains 5000 samples\n",
      "2025-10-09 08:00:53,171 - INFO - Starting sample visualization...\n",
      "INFO: ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "2025-10-09 08:00:53,175 - INFO - ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "2025-10-09 08:00:53,191 - INFO - GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "2025-10-09 08:00:53,192 - INFO - TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "2025-10-09 08:00:53,193 - INFO - HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE SAVE DIR: ./visualizations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-09 08:01:19,536 - ERROR - Visualization failed with error: Expected both inputs to be Half, Float or Double tensors but got ComplexFloat and ComplexFloat\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected both inputs to be Half, Float or Double tensors but got ComplexFloat and ComplexFloat",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 132\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    131\u001b[39m     inference_fn = get_training_loop_by_model_name(model_name, train_loader=test_loader, val_loader=test_loader, test_loader=test_loader, model=model, save_dir=save_dir, mode=args.mode, loss_fn_name=\u001b[33m\"\u001b[39m\u001b[33mmse\u001b[39m\u001b[33m\"\u001b[39m)[\u001b[32m0\u001b[39m].forward\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m     gt, pred, \u001b[38;5;28minput\u001b[39m, orig_gt, orig_pred = \u001b[43mget_full_image_and_prediction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_window\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m5000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m        \u001b[49m\u001b[43mzfile\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m        \u001b[49m\u001b[43minference_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43minference_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_input\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_original\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvminmax\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m6000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    142\u001b[39m     \u001b[38;5;28mprint\u001b[39m(compute_metrics(gt, pred))\n\u001b[32m    143\u001b[39m     display_inference_results(\n\u001b[32m    144\u001b[39m         input_data=\u001b[38;5;28minput\u001b[39m,\n\u001b[32m    145\u001b[39m         gt_data=gt,\n\u001b[32m   (...)\u001b[39m\u001b[32m    150\u001b[39m         save=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    151\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Data/gdaga/sarpyx_new/sarpyx/training/visualize.py:381\u001b[39m, in \u001b[36mget_full_image_and_prediction\u001b[39m\u001b[34m(dataloader, zfile, inference_fn, show_window, return_input, return_original, vminmax, device, verbose)\u001b[39m\n\u001b[32m    379\u001b[39m \u001b[38;5;66;03m# INFERENCE - this is what we're measuring\u001b[39;00m\n\u001b[32m    380\u001b[39m t0 = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m381\u001b[39m pred_batch = \u001b[43minference_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Should return (B, ph, pw) or (B, ph, pw, ...)\u001b[39;00m\n\u001b[32m    382\u001b[39m dt = time.time() - t0\n\u001b[32m    383\u001b[39m tot_inference_time = tot_inference_time + dt\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Data/gdaga/sarpyx_new/sarpyx/training/training_loops.py:632\u001b[39m, in \u001b[36mTrainSSM.forward\u001b[39m\u001b[34m(self, x, y, device)\u001b[39m\n\u001b[32m    630\u001b[39m x_preprocessed = \u001b[38;5;28mself\u001b[39m.preprocess_sample(x, device=device)\n\u001b[32m    631\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.real:\n\u001b[32m--> \u001b[39m\u001b[32m632\u001b[39m     x_preprocessed = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_real_to_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_preprocessed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    633\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mode == \u001b[33m'\u001b[39m\u001b[33mautoregressive\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    634\u001b[39m     y_preprocessed = \u001b[38;5;28mself\u001b[39m.preprocess_sample(y, device=device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Data/gdaga/sarpyx_new/sarpyx/training/training_loops.py:608\u001b[39m, in \u001b[36mTrainSSM._convert_real_to_complex\u001b[39m\u001b[34m(self, student_real_output)\u001b[39m\n\u001b[32m    606\u001b[39m real_part = student_real_output[..., \u001b[32m0\u001b[39m]\n\u001b[32m    607\u001b[39m imag_part = student_real_output[..., \u001b[32m1\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m608\u001b[39m complex_output = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcomplex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_part\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimag_part\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    609\u001b[39m \u001b[38;5;66;03m# Add dimension to match expected format (B, L, 1)\u001b[39;00m\n\u001b[32m    610\u001b[39m complex_output = complex_output.unsqueeze(-\u001b[32m1\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected both inputs to be Half, Float or Double tensors but got ComplexFloat and ComplexFloat"
     ]
    }
   ],
   "source": [
    "def display_inference_results(input_data, gt_data, pred_data, figsize=(20, 6), vminmax=(0, 1000), show: bool=True, save: bool=True, save_path: str=\"./visualizations\"):\n",
    "    \"\"\"\n",
    "    Display input, ground truth, and prediction in a 3-column grid.\n",
    "    \n",
    "    Args:\n",
    "        input_data: Input data from the dataset\n",
    "        gt_data: Ground truth data\n",
    "        pred_data: Model prediction\n",
    "        figsize: Figure size\n",
    "        vminmax: Value range for visualization\n",
    "    \"\"\"\n",
    "    # Convert tensors to numpy if needed\n",
    "    if hasattr(input_data, 'numpy'):\n",
    "        input_data = input_data.cpu().numpy()\n",
    "    if hasattr(gt_data, 'numpy'):\n",
    "        gt_data = gt_data.cpu().numpy()\n",
    "    if hasattr(pred_data, 'numpy'):\n",
    "        pred_data = pred_data.cpu().numpy()\n",
    "    \n",
    "    # Function to get magnitude visualization (similar to get_sample_visualization)\n",
    "    def get_magnitude_vis(data, vminmax):\n",
    "        if np.iscomplexobj(data):\n",
    "            magnitude = np.abs(data)\n",
    "        else:\n",
    "            magnitude = data\n",
    "        \n",
    "        if vminmax == 'auto':\n",
    "            vmin, vmax = np.percentile(magnitude, [2, 98])\n",
    "        elif isinstance(vminmax, tuple):\n",
    "            vmin, vmax = vminmax\n",
    "        else:\n",
    "            vmin, vmax = np.min(magnitude), np.max(magnitude)\n",
    "        \n",
    "        return magnitude, vmin, vmax\n",
    "    \n",
    "    # Prepare visualizations\n",
    "    imgs = []\n",
    "    \n",
    "    # Input data\n",
    "    img, vmin, vmax = get_magnitude_vis(input_data, vminmax)\n",
    "    imgs.append({'name': 'Input (RC)', 'img': img, 'vmin': vmin, 'vmax': vmax})\n",
    "    \n",
    "    # Ground truth\n",
    "    img, vmin, vmax = get_magnitude_vis(gt_data, vminmax)\n",
    "    imgs.append({'name': 'Ground Truth (AZ)', 'img': img, 'vmin': vmin, 'vmax': vmax})\n",
    "    \n",
    "    # Prediction\n",
    "    img, vmin, vmax = get_magnitude_vis(pred_data, vminmax)\n",
    "    imgs.append({'name': 'Prediction (AZ)', 'img': img, 'vmin': vmin, 'vmax': vmax})\n",
    "    \n",
    "    # Create the plot\n",
    "    fig, axes = plt.subplots(1, 3, figsize=figsize)\n",
    "    \n",
    "    for i in range(3):\n",
    "        im = axes[i].imshow(\n",
    "            imgs[i]['img'],\n",
    "            aspect='auto',\n",
    "            cmap='viridis',\n",
    "            vmin=imgs[i]['vmin'],\n",
    "            vmax=imgs[i]['vmax']\n",
    "        )\n",
    "        \n",
    "        axes[i].set_title(f\"{imgs[i]['name']}\")\n",
    "        axes[i].set_xlabel('Range')\n",
    "        axes[i].set_ylabel('Azimuth')\n",
    "        \n",
    "        # Add colorbar\n",
    "        cbar = plt.colorbar(im, ax=axes[i], fraction=0.046, pad=0.04)\n",
    "        cbar.ax.tick_params(labelsize=8)\n",
    "        \n",
    "        # Set equal aspect ratio\n",
    "        axes[i].set_aspect('equal', adjustable='box')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "args = argparse.Namespace(\n",
    "    config=\"training_configs/s4_ssm_student.yaml\",\n",
    "    device=\"cuda\", \n",
    "    batch_size=16,\n",
    "    save_dir=\"./visualizations\",\n",
    "    mode=\"parallel\",\n",
    "    pretrained_path=os.path.join(os.getcwd(), '..', 'results', 'enhanced_distillation','checkpoints', 'last-v2.ckpt'), \n",
    "    learning_rate=1e-4, \n",
    "    num_epochs=50\n",
    ")\n",
    "\n",
    "# # Setup logging\n",
    "logger = setup_logging()\n",
    "#logger.info(f\"Starting visualization with config: {args.config}\")\n",
    "\n",
    "# Load configuration\n",
    "config = load_config(Path(args.config), args)\n",
    "\n",
    "# Extract configurations\n",
    "dataloader_cfg = config['dataloader']\n",
    "training_cfg = config.get('training', {})\n",
    "\n",
    "# Override save directory\n",
    "save_dir = args.save_dir or training_cfg.get('save_dir', './visualizations')\n",
    "\n",
    "# Create test dataloader\n",
    "dataloader_cfg['patch_size'] = [10000, 1]\n",
    "dataloader_cfg['stride'] = [10000, 1]\n",
    "logger.info(\"Configuration Summary:\")\n",
    "logger.info(f\"  Data directory: {dataloader_cfg.get('data_dir', 'Not specified')}\")\n",
    "logger.info(f\"  Level from: {dataloader_cfg.get('level_from', 'rc')}\")\n",
    "logger.info(f\"  Level to: {dataloader_cfg.get('level_to', 'az')}\")\n",
    "logger.info(f\"  Patch size: {dataloader_cfg.get('patch_size', [1000, 1])}\")\n",
    "logger.info(f\"  Batch size: {dataloader_cfg.get('test', {}).get('batch_size', 'Not specified')}\")\n",
    "logger.info(f\"  Save directory: {save_dir}\")\n",
    "logger.info(\"Creating test dataloader...\")\n",
    "try:\n",
    "    test_loader = create_test_dataloader(dataloader_cfg)\n",
    "    logger.info(f\"Created test dataloader with {len(test_loader)} batches\")\n",
    "    logger.info(f\"Dataset contains {len(test_loader.dataset)} samples\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to create test dataloader: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    model = create_model_with_pretrained(config['model'], pretrained_path=args.pretrained_path, device=args.device)\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to load model: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# Visualize samples\n",
    "logger.info(\"Starting sample visualization...\")\n",
    "model_name = config['model']['name']\n",
    "try:\n",
    "    print(f\"Using model: {model}\")\n",
    "    inference_fn = get_training_loop_by_model_name(model_name, train_loader=test_loader, val_loader=test_loader, test_loader=test_loader, model=model, save_dir=save_dir, mode=args.mode, loss_fn_name=\"mse\")[0].forward\n",
    "    gt, pred, input, orig_gt, orig_pred = get_full_image_and_prediction(\n",
    "        dataloader=test_loader,\n",
    "        show_window=((1000, 1000), (10000, 5000)),\n",
    "        zfile=0,\n",
    "        inference_fn=inference_fn,\n",
    "        return_input=True, \n",
    "        return_original=True,\n",
    "        device=\"cuda\", \n",
    "        vminmax=(2000, 6000)\n",
    "    )\n",
    "    print(compute_metrics(gt, pred))\n",
    "    display_inference_results(\n",
    "        input_data=input,\n",
    "        gt_data=gt,\n",
    "        pred_data=pred,\n",
    "        figsize=(20, 6),\n",
    "        vminmax=(2000, 6000),  # Adjust this range based on your data, \n",
    "        show=True, \n",
    "        save=False\n",
    "    )\n",
    "    plot_intensity_histograms(orig_gt[..., 0], orig_pred, gt, pred, figsize=(20, 12), bins=100)\n",
    "    logger.info(\"Visualization completed successfully!\")\n",
    "    logger.info(f\"Check the visualizations in: {save_dir}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Visualization failed with error: {str(e)}\")\n",
    "    raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sar_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
