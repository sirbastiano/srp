{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ae1a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-18 15:07:08,938 - INFO - Configuration Summary:\n",
      "2025-08-18 15:07:08,939 - INFO -   Data directory: /Data/sar_focusing\n",
      "2025-08-18 15:07:08,939 - INFO -   Level from: rcmc\n",
      "2025-08-18 15:07:08,940 - INFO -   Level to: az\n",
      "2025-08-18 15:07:08,940 - INFO -   Patch size: [1000, 1]\n",
      "2025-08-18 15:07:08,941 - INFO -   Batch size: 64\n",
      "2025-08-18 15:07:08,941 - INFO -   Save directory: ./visualizations\n",
      "2025-08-18 15:07:08,941 - INFO - Creating test dataloader...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data_dir': '/Data/sar_focusing', 'epochs': 200, 'lr': '1e-3', 'model': {'name': 'rv_transformer', 'seq_len': 1000, 'input_dim': 2000, 'model_dim': 1024, 'num_layers': 4, 'num_heads': 4, 'ff_dim': 256, 'dropout': 0.1, 'dim_head': 16, 'mode': 'autoregressive', 'dim': 1024, 'depth': 4, 'heads': 4, 'ff_mult': 1}, 'training': {'patience': 20, 'delta': 0.001, 'mode': 'autoregressive', 'device': 'cuda', 'batch_size': 64, 'learning_rate': 0.0001, 'num_epochs': 50, 'save_dir': './visualizations'}, 'dataloader': {'level_from': 'rcmc', 'level_to': 'az', 'num_workers': 0, 'patch_mode': 'rectangular', 'patch_size': [1000, 1], 'buffer': [1000, 1000], 'stride': [300, 1], 'shuffle_files': False, 'complex_valued': False, 'save_samples': False, 'backend': 'zarr', 'verbose': False, 'cache_size': 1000, 'online': True, 'concatenate_patches': True, 'concat_axis': 0, 'positional_encoding': True, 'train': {'batch_size': 64, 'samples_per_prod': 1, 'patch_order': 'row', 'max_products': 1, 'pattern': '*2023*.zarr'}, 'validation': {'batch_size': 64, 'samples_per_prod': 1000, 'patch_order': 'row', 'max_products': 1, 'pattern': '*2024*.zarr'}, 'test': {'batch_size': 64, 'samples_per_prod': 1000, 'patch_order': 'row', 'max_products': 1, 'pattern': '*2025*.zarr'}, 'data_dir': '/Data/sar_focusing', 'transforms': {'normalize': True, 'rc_min': -3000.0, 'rc_max': 3000.0, 'gt_min': -12000.0, 'gt_max': 12000.0}}, 'transforms': {'normalize': True, 'rc_min': -3000.0, 'rc_max': 3000.0, 'gt_min': -12000.0, 'gt_max': 12000.0}, 'device': 'cuda', 'base_save_dir': './results/rv_autoregressive', 'save_results': True}\n",
      "Creating dataloader with config: {'data_dir': '/Data/sar_focusing', 'level_from': 'rcmc', 'level_to': 'az', 'num_workers': 0, 'patch_mode': 'rectangular', 'patch_size': (1000, 1), 'buffer': (1000, 1000), 'stride': (300, 1), 'shuffle_files': False, 'complex_valued': False, 'save_samples': False, 'backend': 'zarr', 'verbose': False, 'cache_size': 1000, 'online': True, 'concatenate_patches': True, 'concat_axis': 0, 'positional_encoding': True, 'transform': None, 'batch_size': 64, 'samples_per_prod': 1000, 'patch_order': 'row', 'max_products': 1, 'file_pattern': '*2025*.zarr'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-18 15:07:11,132 - INFO - Created test dataloader with 0 batches\n",
      "2025-08-18 15:07:11,133 - INFO - Dataset contains 1000 samples\n",
      "2025-08-18 15:07:11,624 - ERROR - Failed to load model: Checkpoint not found: /Data/gdaga/sarpyx_new/sarpyx/training/../results/rv_autoregressive/sar_transform_best.pth\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Checkpoint not found: /Data/gdaga/sarpyx_new/sarpyx/training/../results/rv_autoregressive/sar_transform_best.pth",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 86\u001b[39m\n\u001b[32m     83\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m     model = \u001b[43mcreate_model_with_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpretrained_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     88\u001b[39m     logger.error(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to load model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Data/gdaga/sarpyx_new/sarpyx/model/model_utils.py:646\u001b[39m, in \u001b[36mcreate_model_with_pretrained\u001b[39m\u001b[34m(model_config, pretrained_path, strict_loading, device)\u001b[39m\n\u001b[32m    644\u001b[39m \u001b[38;5;66;03m# Load pretrained weights if provided\u001b[39;00m\n\u001b[32m    645\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pretrained_path:\n\u001b[32m--> \u001b[39m\u001b[32m646\u001b[39m     loading_info = \u001b[43mload_pretrained_weights\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpretrained_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrict_loading\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    653\u001b[39m     \u001b[38;5;66;03m# Store loading info as model attribute for reference\u001b[39;00m\n\u001b[32m    654\u001b[39m     model._pretrained_info = loading_info\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Data/gdaga/sarpyx_new/sarpyx/model/model_utils.py:560\u001b[39m, in \u001b[36mload_pretrained_weights\u001b[39m\u001b[34m(model, checkpoint_path, strict, map_location, verbose)\u001b[39m\n\u001b[32m    558\u001b[39m checkpoint_path_obj = Path(checkpoint_path)\n\u001b[32m    559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m checkpoint_path_obj.exists():\n\u001b[32m--> \u001b[39m\u001b[32m560\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCheckpoint not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_path_obj\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    562\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[32m    563\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading pretrained weights from: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_path_obj\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: Checkpoint not found: /Data/gdaga/sarpyx_new/sarpyx/training/../results/rv_autoregressive/sar_transform_best.pth"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "import os \n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "import yaml\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import logging\n",
    "import time\n",
    "from typing import Dict, Any\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dataloader.dataloader import get_sar_dataloader, SARTransform\n",
    "from model.model_utils import get_model_from_configs, create_model_with_pretrained\n",
    "from training.training_loops import TrainRVTransformer, TrainCVTransformer, TrainSSM\n",
    "from training.visualize import save_results_and_metrics\n",
    "from sarpyx.utils.losses import get_loss_function\n",
    "from training_script import setup_logging, load_config\n",
    "from inference_script import create_test_dataloader, visualize_batch_samples\n",
    "# #parser = argparse.ArgumentParser(description='Visualize SAR data samples from test dataloader')\n",
    "# config = \"rv_transformer_autoregressive.yaml\"\n",
    "# #parser.add_argument('--config', type=str, default=\"rv_transformer_autoregressive.yaml\", help='Path to configuration file')\n",
    "# device = \"cuda\"\n",
    "# #parser.add_argument('--device', type=str, default='cpu', help='Device override (cpu/cuda)')\n",
    "# batch_size = 64\n",
    "# #parser.add_argument('--batch_size', type=int, default=None, help='Batch size override')\n",
    "# save_dir = \"./visualizations\"\n",
    "# #parser.add_argument('--save_dir', type=str, default='./visualizations', help='Save directory for visualizations')\n",
    "# max_batches = 64\n",
    "# #parser.add_argument('--max_batches', type=int, default=5, help='Maximum number of batches to visualize')\n",
    "# max_samples_per_batch = 4\n",
    "# #parser.add_argument('--max_samples_per_batch', type=int, default=4, help='Maximum samples per batch')\n",
    "\n",
    "# #args = parser.parse_args()\n",
    "# pretrained_path = os.path.join(os.getcwd(), '..', 'results', 'rv_autoregressive','sar_transform_best.pth')\n",
    "\n",
    "args = argparse.Namespace(\n",
    "    config=\"rv_transformer_autoregressive.yaml\",\n",
    "    device=\"cuda\", \n",
    "    batch_size=64,\n",
    "    save_dir=\"./visualizations\",\n",
    "    max_batches=64,\n",
    "    max_samples_per_batch=4,\n",
    "    mode=\"autoregressive\",\n",
    "    pretrained_path=os.path.join(os.getcwd(), '..', 'results', 'rv_autoregressive','sar_transformer_best.pth'), \n",
    "    learning_rate=1e-4, \n",
    "    num_epochs=50\n",
    ")\n",
    "\n",
    "# # Setup logging\n",
    "logger = setup_logging()\n",
    "#logger.info(f\"Starting visualization with config: {args.config}\")\n",
    "\n",
    "# Load configuration\n",
    "config = load_config(Path(args.config), args)\n",
    "\n",
    "# Extract configurations\n",
    "dataloader_cfg = config['dataloader']\n",
    "training_cfg = config.get('training', {})\n",
    "\n",
    "# Override save directory\n",
    "save_dir = args.save_dir or training_cfg.get('save_dir', './visualizations')\n",
    "\n",
    "# Log configuration summary\n",
    "logger.info(\"Configuration Summary:\")\n",
    "logger.info(f\"  Data directory: {dataloader_cfg.get('data_dir', 'Not specified')}\")\n",
    "logger.info(f\"  Level from: {dataloader_cfg.get('level_from', 'rcmc')}\")\n",
    "logger.info(f\"  Level to: {dataloader_cfg.get('level_to', 'az')}\")\n",
    "logger.info(f\"  Patch size: {dataloader_cfg.get('patch_size', [1000, 1])}\")\n",
    "logger.info(f\"  Batch size: {dataloader_cfg.get('test', {}).get('batch_size', 'Not specified')}\")\n",
    "logger.info(f\"  Save directory: {save_dir}\")\n",
    "\n",
    "# Create test dataloader\n",
    "logger.info(\"Creating test dataloader...\")\n",
    "try:\n",
    "    test_loader = create_test_dataloader(dataloader_cfg)\n",
    "    logger.info(f\"Created test dataloader with {len(test_loader)} batches\")\n",
    "    logger.info(f\"Dataset contains {len(test_loader.dataset)} samples\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to create test dataloader: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    model = create_model_with_pretrained(config['model'], pretrained_path=args.pretrained_path, device=args.device)\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to load model: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# Visualize samples\n",
    "logger.info(\"Starting sample visualization...\")\n",
    "try:\n",
    "    visualize_batch_samples(\n",
    "        model=model,\n",
    "        test_loader=test_loader,\n",
    "        save_dir=save_dir,\n",
    "        max_batches=args.max_batches,\n",
    "        max_samples_per_batch=args.max_samples_per_batch\n",
    "    )\n",
    "    \n",
    "    logger.info(\"Visualization completed successfully!\")\n",
    "    logger.info(f\"Check the visualizations in: {save_dir}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Visualization failed with error: {str(e)}\")\n",
    "    raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sar_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
