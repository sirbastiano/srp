{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98960520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable autoreload to automatically reload modules when they are updated\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7b573f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Dict, Any, Optional, Union\n",
    "import joblib\n",
    "import psutil\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Only add path if not already added\n",
    "cwd = Path.cwd()\n",
    "sarpyx_path = cwd.parent\n",
    "if str(sarpyx_path) not in __import__('sys').path:\n",
    "    __import__('sys').path.append(str(sarpyx_path))\n",
    "\n",
    "# Import after adding path to avoid import errors\n",
    "try:\n",
    "    from sarpyx.processor.core.focus import CoarseRDA\n",
    "    if 'CoarseRDA' not in globals() or 'raw_data' not in globals():\n",
    "        print('‚úÖ Successfully imported CoarseRDA')\n",
    "except ImportError as e:\n",
    "    print(f'‚ùå Import error: {e}')\n",
    "    raise\n",
    "    \n",
    "def get_processed_data_paths(base_dir: Path, burst_idx: int) -> Dict[str, Path]:\n",
    "    \"\"\"Get the paths for echo, metadata, and ephemeris files in the processed_data directory.\n",
    "\n",
    "    Args:\n",
    "        base_dir (Path): The base directory containing the 'processed_data' folder.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Path]: Dictionary with keys 'echo', 'metadata', and 'ephemeris' and their corresponding file paths.\n",
    "\n",
    "    Raises:\n",
    "        AssertionError: If any of the required files are not found.\n",
    "    \"\"\"\n",
    "    processed_data_dir = base_dir / 'processed_data'\n",
    "    assert processed_data_dir.exists(), f'Processed data directory not found: {processed_data_dir}'\n",
    "\n",
    "    # Find files with the specified burst index pattern\n",
    "    echo_pattern = f'*_burst_{burst_idx}_echo.pkl'\n",
    "    metadata_pattern = f'*_burst_{burst_idx}_metadata.pkl'\n",
    "    ephemeris_pattern = '*_ephemeris.pkl'  # Ephemeris files don't have burst index\n",
    "    \n",
    "    echo_path = next(processed_data_dir.glob(echo_pattern), None)\n",
    "    metadata_path = next(processed_data_dir.glob(metadata_pattern), None)\n",
    "    ephemeris_path = next(processed_data_dir.glob(ephemeris_pattern), None)\n",
    "\n",
    "    assert echo_path is not None, 'Echo file not found in processed_data directory.'\n",
    "    assert metadata_path is not None, 'Metadata file not found in processed_data directory.'\n",
    "    assert ephemeris_path is not None, 'Ephemeris file not found in processed_data directory.'\n",
    "\n",
    "    return {\n",
    "        'echo': echo_path,\n",
    "        'metadata': metadata_path,\n",
    "        'ephemeris': ephemeris_path,\n",
    "    }\n",
    "\n",
    "# Only load data if not already loaded\n",
    "if 'raw_data' not in globals():\n",
    "    print('üìÅ Loading processed data files...')\n",
    "    paths = get_processed_data_paths(cwd.parent, 1)\n",
    "    echo_path = paths['echo']\n",
    "    metadata_path = paths['metadata']\n",
    "    ephemeris_path = paths['ephemeris']\n",
    "\n",
    "    echo = joblib.load(echo_path)\n",
    "    metadata = joblib.load(metadata_path)\n",
    "    ephemeris = joblib.load(ephemeris_path)\n",
    "\n",
    "    raw_data = {\n",
    "        'echo': echo,\n",
    "        'metadata': metadata,\n",
    "        'ephemeris': ephemeris,\n",
    "    }\n",
    "\n",
    "    print('‚úÖ Loaded data successfully:')\n",
    "    print(f'  Echo shape: {echo.shape}')\n",
    "    print(f'  Metadata shape: {metadata.shape}')\n",
    "    print(f'  Ephemeris shape: {ephemeris.shape}')\n",
    "else:\n",
    "    print('‚úÖ Data already loaded and available in workspace')\n",
    "# Initialize the focuser\n",
    "try:\n",
    "    focuser = CoarseRDA(raw_data=raw_data,\n",
    "                        verbose=True,\n",
    "                        backend='numpy',\n",
    "                    )\n",
    "    print('‚úÖ Focuser initialized successfully!')\n",
    "    print(f'  Radar data shape: {focuser.radar_data.shape}')\n",
    "    print(f'  Backend: {focuser._backend}')\n",
    "except Exception as e:\n",
    "    print(f'‚ùå Error initializing focuser: {e}')\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72784bd",
   "metadata": {},
   "source": [
    "# SAR Focusing API\n",
    "\n",
    "This section provides a minimal API to focus the SAR product using the CoarseRDA processor.\n",
    "The API includes functions for:\n",
    "- Focusing SAR products\n",
    "- Displaying results\n",
    "- Memory management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcec0455",
   "metadata": {},
   "outputs": [],
   "source": [
    "def focus_sar_product(\n",
    "    focuser: CoarseRDA, \n",
    "    save_path: Optional[Union[str, Path]] = None\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Focus SAR product using Range Doppler Algorithm.\n",
    "    \n",
    "    Args:\n",
    "        focuser (CoarseRDA): Initialized CoarseRDA processor instance.\n",
    "        save_path (Optional[Union[str, Path]]): Optional path to save the focused data.\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: Focused radar data array.\n",
    "        \n",
    "    Raises:\n",
    "        AssertionError: If focuser is not properly initialized.\n",
    "        Exception: If focusing process fails.\n",
    "    \"\"\"\n",
    "    assert isinstance(focuser, CoarseRDA), 'focuser must be a CoarseRDA instance'\n",
    "    assert hasattr(focuser, 'radar_data'), 'Focuser must be initialized with radar data'\n",
    "    assert focuser.radar_data is not None, 'Radar data cannot be None'\n",
    "    \n",
    "    print('üöÄ Starting SAR product focusing...')\n",
    "    print(f'Input data shape: {focuser.radar_data.shape}')\n",
    "    print(f'Backend: {focuser._backend}')\n",
    "    \n",
    "    try:\n",
    "        # Run the focusing algorithm\n",
    "        focuser.data_focus()\n",
    "        \n",
    "        # Save if path provided\n",
    "        if save_path:\n",
    "            save_path = Path(save_path) if isinstance(save_path, str) else save_path\n",
    "            focuser.save_file(save_path)\n",
    "            print(f'‚úÖ Focused data saved to: {save_path}')\n",
    "        \n",
    "        print('‚úÖ SAR focusing completed successfully!')\n",
    "        return focuser.radar_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'‚ùå Error during focusing: {str(e)}')\n",
    "        print(f'üìä Debug info:')\n",
    "        print(f'  Radar data shape: {focuser.radar_data.shape}')\n",
    "        if hasattr(focuser, 'effective_velocities'):\n",
    "            print(f'  Effective velocities shape: {focuser.effective_velocities.shape}')\n",
    "        if hasattr(focuser, 'az_freq_vals'):\n",
    "            print(f'  Azimuth freq vals shape: {focuser.az_freq_vals.shape}')\n",
    "        if hasattr(focuser, 'D'):\n",
    "            print(f'  D array shape: {focuser.D.shape}')\n",
    "        if hasattr(focuser, 'slant_range_vec'):\n",
    "            print(f'  Slant range vec shape: {focuser.slant_range_vec.shape}')\n",
    "        raise\n",
    "\n",
    "\n",
    "def get_focusing_metadata(focuser: CoarseRDA) -> Dict[str, Any]:\n",
    "    \"\"\"Extract metadata information from the CoarseRDA processor.\n",
    "    \n",
    "    Args:\n",
    "        focuser (CoarseRDA): Initialized CoarseRDA processor instance.\n",
    "        \n",
    "    Returns:\n",
    "        Dict[str, Any]: Dictionary containing processing metadata.\n",
    "    \"\"\"\n",
    "    metadata = {\n",
    "        'backend': focuser._backend,\n",
    "        'verbose': focuser._verbose,\n",
    "        'data_shape': focuser.radar_data.shape,\n",
    "        'range_sample_freq': getattr(focuser, 'range_sample_freq', None),\n",
    "        'replica_length': getattr(focuser, 'replica_len', None),\n",
    "        'azimuth_lines': focuser.len_az_line,\n",
    "        'range_lines': focuser.len_range_line,\n",
    "    }\n",
    "    \n",
    "    # Add optional attributes if they exist\n",
    "    optional_attrs = ['wavelength', 'pri', 'c', 'az_sample_freq']\n",
    "    for attr in optional_attrs:\n",
    "        if hasattr(focuser, attr):\n",
    "            metadata[attr] = getattr(focuser, attr)\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "\n",
    "# Run the focusing process\n",
    "print('üìÅ Setting up output path...')\n",
    "focused_data_path = cwd.parent / 'processed_data' / 'focused_sar_data.pkl'\n",
    "focused_data_path.parent.mkdir(exist_ok=True)  # Ensure directory exists\n",
    "\n",
    "print('üìä Processor metadata before focusing:')\n",
    "pre_focus_metadata = get_focusing_metadata(focuser)\n",
    "for key, value in pre_focus_metadata.items():\n",
    "    print(f'  {key}: {value}')\n",
    "\n",
    "focused_radar_data = focus_sar_product(focuser, focused_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112d5506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_focusing_results(radar_data: np.ndarray, metadata: Optional[Dict[str, Any]] = None) -> None:\n",
    "    \"\"\"Display comprehensive information about the focused radar data.\n",
    "    \n",
    "    Args:\n",
    "        radar_data (np.ndarray): Focused radar data array.\n",
    "        metadata (Optional[Dict[str, Any]]): Optional metadata dictionary.\n",
    "    \"\"\"\n",
    "    print('=' * 50)\n",
    "    print('üìà SAR FOCUSING RESULTS')\n",
    "    print('=' * 50)\n",
    "    \n",
    "    # Basic data information\n",
    "    print('üìä Data Information:')\n",
    "    print(f'  Shape: {radar_data.shape}')\n",
    "    print(f'  Data type: {radar_data.dtype}')\n",
    "    print(f'  Size: {radar_data.nbytes / (1024**2):.2f} MB')\n",
    "    print(f'  Complex data: {np.iscomplexobj(radar_data)}')\n",
    "    \n",
    "    # Statistical information\n",
    "    print('\\nüìà Statistical Analysis:')\n",
    "    if np.iscomplexobj(radar_data):\n",
    "        amplitudes = np.abs(radar_data)\n",
    "        phases = np.angle(radar_data)\n",
    "        \n",
    "        print(f'  Max amplitude: {amplitudes.max():.6f}')\n",
    "        print(f'  Mean amplitude: {amplitudes.mean():.6f}')\n",
    "        print(f'  Min amplitude: {amplitudes.min():.6f}')\n",
    "        print(f'  Std amplitude: {amplitudes.std():.6f}')\n",
    "        print(f'  Phase range: [{phases.min():.3f}, {phases.max():.3f}] rad')\n",
    "    else:\n",
    "        print(f'  Max value: {radar_data.max():.6f}')\n",
    "        print(f'  Mean value: {radar_data.mean():.6f}')\n",
    "        print(f'  Min value: {radar_data.min():.6f}')\n",
    "        print(f'  Std value: {radar_data.std():.6f}')\n",
    "    \n",
    "    # Data quality checks\n",
    "    print('\\nüîç Data Quality Checks:')\n",
    "    nan_count = np.isnan(radar_data).sum()\n",
    "    inf_count = np.isinf(radar_data).sum()\n",
    "    zero_count = (radar_data == 0).sum()\n",
    "    \n",
    "    if nan_count > 0:\n",
    "        print(f'  ‚ö†Ô∏è  Warning: {nan_count} NaN values detected ({nan_count/radar_data.size*100:.2f}%)')\n",
    "    else:\n",
    "        print('  ‚úÖ No NaN values detected')\n",
    "        \n",
    "    if inf_count > 0:\n",
    "        print(f'  ‚ö†Ô∏è  Warning: {inf_count} infinite values detected ({inf_count/radar_data.size*100:.2f}%)')\n",
    "    else:\n",
    "        print('  ‚úÖ No infinite values detected')\n",
    "        \n",
    "    if zero_count > radar_data.size * 0.1:  # More than 10% zeros might be suspicious\n",
    "        print(f'  ‚ö†Ô∏è  Note: {zero_count} zero values detected ({zero_count/radar_data.size*100:.2f}%)')\n",
    "    else:\n",
    "        print(f'  ‚úÖ Zero values: {zero_count} ({zero_count/radar_data.size*100:.2f}%)')\n",
    "    \n",
    "    # Processing metadata\n",
    "    if metadata:\n",
    "        print('\\n‚öôÔ∏è  Processing Metadata:')\n",
    "        for key, value in metadata.items():\n",
    "            print(f'  {key}: {value}')\n",
    "    \n",
    "    print('=' * 50)\n",
    "\n",
    "\n",
    "def analyze_focusing_quality(radar_data: np.ndarray) -> Dict[str, float]:\n",
    "    \"\"\"Analyze the quality of the focused SAR data.\n",
    "    \n",
    "    Args:\n",
    "        radar_data (np.ndarray): Focused radar data array.\n",
    "        \n",
    "    Returns:\n",
    "        Dict[str, float]: Dictionary containing quality metrics.\n",
    "    \"\"\"\n",
    "    if not np.iscomplexobj(radar_data):\n",
    "        raise ValueError('Expected complex radar data for quality analysis')\n",
    "    \n",
    "    amplitudes = np.abs(radar_data)\n",
    "    \n",
    "    # Calculate quality metrics\n",
    "    quality_metrics = {\n",
    "        'peak_to_mean_ratio': amplitudes.max() / amplitudes.mean(),\n",
    "        'dynamic_range_db': 20 * np.log10(amplitudes.max() / amplitudes.min()) if amplitudes.min() > 0 else float('inf'),\n",
    "        'signal_to_noise_estimate': amplitudes.mean() / amplitudes.std(),\n",
    "        'image_entropy': -np.sum(amplitudes * np.log(amplitudes + 1e-12)),\n",
    "        'contrast': amplitudes.std() / amplitudes.mean(),\n",
    "    }\n",
    "    \n",
    "    return quality_metrics\n",
    "\n",
    "\n",
    "# Display comprehensive results only if focusing was successful\n",
    "if 'focused_radar_data' in locals():\n",
    "    post_focus_metadata = get_focusing_metadata(focuser)\n",
    "    display_focusing_results(focused_radar_data, post_focus_metadata)\n",
    "    \n",
    "    # Analyze focusing quality\n",
    "    print('\\nüéØ Focusing Quality Analysis:')\n",
    "    quality_metrics = analyze_focusing_quality(focused_radar_data)\n",
    "    for metric, value in quality_metrics.items():\n",
    "        print(f'  {metric}: {value:.4f}')\n",
    "else:\n",
    "    print('‚è∏Ô∏è  Focusing not yet completed - run the focusing cell first.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2574c695",
   "metadata": {},
   "source": [
    "# Viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efd6d98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sarpyx.processor.utils.viz import plot_with_cdf\n",
    "import joblib \n",
    "# load pkl file\n",
    "focused_data_path = \"/Data_large/marine/PythonProjects/SAR/sarpyx/processed_data/focused_sar_data.pkl\"\n",
    "focused_data = joblib.load(focused_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3505cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "focused_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e509e5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_with_cdf(focused_data[:5000,:5000], savepath=\"img.jpg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
