{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98960520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable autoreload to automatically reload modules when they are updated\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef7b573f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully imported CoarseRDA\n",
      "üìÅ Loading processed data files...\n",
      "‚úÖ Loaded data successfully:\n",
      "  Echo shape: (56130, 25724)\n",
      "  Metadata shape: (56130, 53)\n",
      "  Ephemeris shape: (881, 59)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from typing import Dict, Any, Optional, Union\n",
    "import joblib\n",
    "import psutil\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Only add path if not already added\n",
    "cwd = Path.cwd()\n",
    "sarpyx_path = cwd.parent\n",
    "if str(sarpyx_path) not in __import__('sys').path:\n",
    "    __import__('sys').path.append(str(sarpyx_path))\n",
    "\n",
    "# Import after adding path to avoid import errors\n",
    "try:\n",
    "    from sarpyx.processor.core.focus import CoarseRDA\n",
    "    if 'CoarseRDA' not in globals() or 'raw_data' not in globals():\n",
    "        print('‚úÖ Successfully imported CoarseRDA')\n",
    "except ImportError as e:\n",
    "    print(f'‚ùå Import error: {e}')\n",
    "    raise\n",
    "    \n",
    "def get_processed_data_paths(base_dir: Path) -> Dict[str, Path]:\n",
    "    \"\"\"Get the paths for echo, metadata, and ephemeris files in the processed_data directory.\n",
    "\n",
    "    Args:\n",
    "        base_dir (Path): The base directory containing the 'processed_data' folder.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Path]: Dictionary with keys 'echo', 'metadata', and 'ephemeris' and their corresponding file paths.\n",
    "\n",
    "    Raises:\n",
    "        AssertionError: If any of the required files are not found.\n",
    "    \"\"\"\n",
    "    processed_data_dir = base_dir / 'processed_data'\n",
    "    assert processed_data_dir.exists(), f'Processed data directory not found: {processed_data_dir}'\n",
    "\n",
    "    echo_path = next(processed_data_dir.glob('*_echo.pkl'), None)\n",
    "    metadata_path = next(processed_data_dir.glob('*_metadata.pkl'), None)\n",
    "    ephemeris_path = next(processed_data_dir.glob('*_ephemeris.pkl'), None)\n",
    "\n",
    "    assert echo_path is not None, 'Echo file not found in processed_data directory.'\n",
    "    assert metadata_path is not None, 'Metadata file not found in processed_data directory.'\n",
    "    assert ephemeris_path is not None, 'Ephemeris file not found in processed_data directory.'\n",
    "\n",
    "    return {\n",
    "        'echo': echo_path,\n",
    "        'metadata': metadata_path,\n",
    "        'ephemeris': ephemeris_path,\n",
    "    }\n",
    "\n",
    "# Only load data if not already loaded\n",
    "if 'raw_data' not in globals():\n",
    "    print('üìÅ Loading processed data files...')\n",
    "    paths = get_processed_data_paths(cwd.parent)\n",
    "    echo_path = paths['echo']\n",
    "    metadata_path = paths['metadata']\n",
    "    ephemeris_path = paths['ephemeris']\n",
    "\n",
    "    echo = joblib.load(echo_path)\n",
    "    metadata = joblib.load(metadata_path)\n",
    "    ephemeris = joblib.load(ephemeris_path)\n",
    "\n",
    "    raw_data = {\n",
    "        'echo': echo,\n",
    "        'metadata': metadata,\n",
    "        'ephemeris': ephemeris,\n",
    "    }\n",
    "\n",
    "    print('‚úÖ Loaded data successfully:')\n",
    "    print(f'  Echo shape: {echo.shape}')\n",
    "    print(f'  Metadata shape: {metadata.shape}')\n",
    "    print(f'  Ephemeris shape: {ephemeris.shape}')\n",
    "else:\n",
    "    print('‚úÖ Data already loaded and available in workspace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a9b4017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current memory usage: 11420.19 MB\n"
     ]
    }
   ],
   "source": [
    "def print_memory_usage() -> None:\n",
    "    \"\"\"Print the current memory usage of the Python process.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Raises:\n",
    "        AssertionError: If memory usage cannot be retrieved.\n",
    "    \"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem_bytes = process.memory_info().rss\n",
    "    assert mem_bytes > 0, 'Memory usage could not be determined.'\n",
    "    mem_mb = mem_bytes / (1024 ** 2)\n",
    "    print(f'Current memory usage: {mem_mb:.2f} MB')\n",
    "\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a65a9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded radar data with shape: (56130, 25724)\n",
      "Azimuth lines: 56130, Range lines: 25724\n",
      "Generating transmission replica...\n",
      "Range decimation code: 1\n",
      "Range sample frequency: 100092592.64 Hz\n",
      "TX pulse start frequency: -43801345.00 Hz\n",
      "TX ramp rate: 1927378686407.00 Hz/s\n",
      "TX pulse length: 0.000045 s\n",
      "Number of TX values: 4549\n",
      "Phase parameters - phi1: -460.87, phi2: 9.64e+11\n",
      "Replica length: 4549\n",
      "Transmission replica generated successfully!\n",
      "Elapsed time for _generate_tx_replica: 0.0505 seconds\n",
      "‚úÖ Focuser initialized successfully!\n",
      "  Radar data shape: (56130, 25724)\n",
      "  Backend: numpy\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Initialize the focuser\n",
    "try:\n",
    "    focuser = CoarseRDA(raw_data=raw_data,\n",
    "                        verbose=True,\n",
    "                        backend='numpy',\n",
    "                    )\n",
    "    print('‚úÖ Focuser initialized successfully!')\n",
    "    print(f'  Radar data shape: {focuser.radar_data.shape}')\n",
    "    print(f'  Backend: {focuser._backend}')\n",
    "except Exception as e:\n",
    "    print(f'‚ùå Error initializing focuser: {e}')\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72784bd",
   "metadata": {},
   "source": [
    "# SAR Focusing API\n",
    "\n",
    "This section provides a minimal API to focus the SAR product using the CoarseRDA processor.\n",
    "The API includes functions for:\n",
    "- Focusing SAR products\n",
    "- Displaying results\n",
    "- Memory management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcec0455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Setting up output path...\n",
      "üìä Processor metadata before focusing:\n",
      "  backend: numpy\n",
      "  verbose: True\n",
      "  data_shape: (56130, 25724)\n",
      "  range_sample_freq: 100092592.64\n",
      "  replica_length: 4549\n",
      "  azimuth_lines: 56130\n",
      "  range_lines: 25724\n",
      "üöÄ Starting SAR product focusing...\n",
      "Input data shape: (56130, 25724)\n",
      "Backend: numpy\n",
      "Starting SAR data focusing...\n",
      "Initial radar data shape: (56130, 25724)\n",
      "Processing with w_pad=4549, original_w=25724\n",
      "FFT input data shape: (56130, 25724)\n"
     ]
    }
   ],
   "source": [
    "def focus_sar_product(\n",
    "    focuser: CoarseRDA, \n",
    "    save_path: Optional[Union[str, Path]] = None\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Focus SAR product using Range Doppler Algorithm.\n",
    "    \n",
    "    Args:\n",
    "        focuser (CoarseRDA): Initialized CoarseRDA processor instance.\n",
    "        save_path (Optional[Union[str, Path]]): Optional path to save the focused data.\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: Focused radar data array.\n",
    "        \n",
    "    Raises:\n",
    "        AssertionError: If focuser is not properly initialized.\n",
    "        Exception: If focusing process fails.\n",
    "    \"\"\"\n",
    "    assert isinstance(focuser, CoarseRDA), 'focuser must be a CoarseRDA instance'\n",
    "    assert hasattr(focuser, 'radar_data'), 'Focuser must be initialized with radar data'\n",
    "    assert focuser.radar_data is not None, 'Radar data cannot be None'\n",
    "    \n",
    "    print('üöÄ Starting SAR product focusing...')\n",
    "    print(f'Input data shape: {focuser.radar_data.shape}')\n",
    "    print(f'Backend: {focuser._backend}')\n",
    "    \n",
    "    try:\n",
    "        # Run the focusing algorithm\n",
    "        focuser.data_focus()\n",
    "        \n",
    "        # Save if path provided\n",
    "        if save_path:\n",
    "            save_path = Path(save_path) if isinstance(save_path, str) else save_path\n",
    "            focuser.save_file(save_path)\n",
    "            print(f'‚úÖ Focused data saved to: {save_path}')\n",
    "        \n",
    "        print('‚úÖ SAR focusing completed successfully!')\n",
    "        return focuser.radar_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'‚ùå Error during focusing: {str(e)}')\n",
    "        print(f'üìä Debug info:')\n",
    "        print(f'  Radar data shape: {focuser.radar_data.shape}')\n",
    "        if hasattr(focuser, 'effective_velocities'):\n",
    "            print(f'  Effective velocities shape: {focuser.effective_velocities.shape}')\n",
    "        if hasattr(focuser, 'az_freq_vals'):\n",
    "            print(f'  Azimuth freq vals shape: {focuser.az_freq_vals.shape}')\n",
    "        if hasattr(focuser, 'D'):\n",
    "            print(f'  D array shape: {focuser.D.shape}')\n",
    "        if hasattr(focuser, 'slant_range_vec'):\n",
    "            print(f'  Slant range vec shape: {focuser.slant_range_vec.shape}')\n",
    "        raise\n",
    "\n",
    "\n",
    "def get_focusing_metadata(focuser: CoarseRDA) -> Dict[str, Any]:\n",
    "    \"\"\"Extract metadata information from the CoarseRDA processor.\n",
    "    \n",
    "    Args:\n",
    "        focuser (CoarseRDA): Initialized CoarseRDA processor instance.\n",
    "        \n",
    "    Returns:\n",
    "        Dict[str, Any]: Dictionary containing processing metadata.\n",
    "    \"\"\"\n",
    "    metadata = {\n",
    "        'backend': focuser._backend,\n",
    "        'verbose': focuser._verbose,\n",
    "        'data_shape': focuser.radar_data.shape,\n",
    "        'range_sample_freq': getattr(focuser, 'range_sample_freq', None),\n",
    "        'replica_length': getattr(focuser, 'replica_len', None),\n",
    "        'azimuth_lines': focuser.len_az_line,\n",
    "        'range_lines': focuser.len_range_line,\n",
    "    }\n",
    "    \n",
    "    # Add optional attributes if they exist\n",
    "    optional_attrs = ['wavelength', 'pri', 'c', 'az_sample_freq']\n",
    "    for attr in optional_attrs:\n",
    "        if hasattr(focuser, attr):\n",
    "            metadata[attr] = getattr(focuser, attr)\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "\n",
    "# Run the focusing process\n",
    "print('üìÅ Setting up output path...')\n",
    "focused_data_path = cwd.parent / 'processed_data' / 'focused_sar_data.pkl'\n",
    "focused_data_path.parent.mkdir(exist_ok=True)  # Ensure directory exists\n",
    "\n",
    "print('üìä Processor metadata before focusing:')\n",
    "pre_focus_metadata = get_focusing_metadata(focuser)\n",
    "for key, value in pre_focus_metadata.items():\n",
    "    print(f'  {key}: {value}')\n",
    "\n",
    "focused_radar_data = focus_sar_product(focuser, focused_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112d5506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_focusing_results(radar_data: np.ndarray, metadata: Optional[Dict[str, Any]] = None) -> None:\n",
    "    \"\"\"Display comprehensive information about the focused radar data.\n",
    "    \n",
    "    Args:\n",
    "        radar_data (np.ndarray): Focused radar data array.\n",
    "        metadata (Optional[Dict[str, Any]]): Optional metadata dictionary.\n",
    "    \"\"\"\n",
    "    print('=' * 50)\n",
    "    print('üìà SAR FOCUSING RESULTS')\n",
    "    print('=' * 50)\n",
    "    \n",
    "    # Basic data information\n",
    "    print('üìä Data Information:')\n",
    "    print(f'  Shape: {radar_data.shape}')\n",
    "    print(f'  Data type: {radar_data.dtype}')\n",
    "    print(f'  Size: {radar_data.nbytes / (1024**2):.2f} MB')\n",
    "    print(f'  Complex data: {np.iscomplexobj(radar_data)}')\n",
    "    \n",
    "    # Statistical information\n",
    "    print('\\nüìà Statistical Analysis:')\n",
    "    if np.iscomplexobj(radar_data):\n",
    "        amplitudes = np.abs(radar_data)\n",
    "        phases = np.angle(radar_data)\n",
    "        \n",
    "        print(f'  Max amplitude: {amplitudes.max():.6f}')\n",
    "        print(f'  Mean amplitude: {amplitudes.mean():.6f}')\n",
    "        print(f'  Min amplitude: {amplitudes.min():.6f}')\n",
    "        print(f'  Std amplitude: {amplitudes.std():.6f}')\n",
    "        print(f'  Phase range: [{phases.min():.3f}, {phases.max():.3f}] rad')\n",
    "    else:\n",
    "        print(f'  Max value: {radar_data.max():.6f}')\n",
    "        print(f'  Mean value: {radar_data.mean():.6f}')\n",
    "        print(f'  Min value: {radar_data.min():.6f}')\n",
    "        print(f'  Std value: {radar_data.std():.6f}')\n",
    "    \n",
    "    # Data quality checks\n",
    "    print('\\nüîç Data Quality Checks:')\n",
    "    nan_count = np.isnan(radar_data).sum()\n",
    "    inf_count = np.isinf(radar_data).sum()\n",
    "    zero_count = (radar_data == 0).sum()\n",
    "    \n",
    "    if nan_count > 0:\n",
    "        print(f'  ‚ö†Ô∏è  Warning: {nan_count} NaN values detected ({nan_count/radar_data.size*100:.2f}%)')\n",
    "    else:\n",
    "        print('  ‚úÖ No NaN values detected')\n",
    "        \n",
    "    if inf_count > 0:\n",
    "        print(f'  ‚ö†Ô∏è  Warning: {inf_count} infinite values detected ({inf_count/radar_data.size*100:.2f}%)')\n",
    "    else:\n",
    "        print('  ‚úÖ No infinite values detected')\n",
    "        \n",
    "    if zero_count > radar_data.size * 0.1:  # More than 10% zeros might be suspicious\n",
    "        print(f'  ‚ö†Ô∏è  Note: {zero_count} zero values detected ({zero_count/radar_data.size*100:.2f}%)')\n",
    "    else:\n",
    "        print(f'  ‚úÖ Zero values: {zero_count} ({zero_count/radar_data.size*100:.2f}%)')\n",
    "    \n",
    "    # Processing metadata\n",
    "    if metadata:\n",
    "        print('\\n‚öôÔ∏è  Processing Metadata:')\n",
    "        for key, value in metadata.items():\n",
    "            print(f'  {key}: {value}')\n",
    "    \n",
    "    print('=' * 50)\n",
    "\n",
    "\n",
    "def analyze_focusing_quality(radar_data: np.ndarray) -> Dict[str, float]:\n",
    "    \"\"\"Analyze the quality of the focused SAR data.\n",
    "    \n",
    "    Args:\n",
    "        radar_data (np.ndarray): Focused radar data array.\n",
    "        \n",
    "    Returns:\n",
    "        Dict[str, float]: Dictionary containing quality metrics.\n",
    "    \"\"\"\n",
    "    if not np.iscomplexobj(radar_data):\n",
    "        raise ValueError('Expected complex radar data for quality analysis')\n",
    "    \n",
    "    amplitudes = np.abs(radar_data)\n",
    "    \n",
    "    # Calculate quality metrics\n",
    "    quality_metrics = {\n",
    "        'peak_to_mean_ratio': amplitudes.max() / amplitudes.mean(),\n",
    "        'dynamic_range_db': 20 * np.log10(amplitudes.max() / amplitudes.min()) if amplitudes.min() > 0 else float('inf'),\n",
    "        'signal_to_noise_estimate': amplitudes.mean() / amplitudes.std(),\n",
    "        'image_entropy': -np.sum(amplitudes * np.log(amplitudes + 1e-12)),\n",
    "        'contrast': amplitudes.std() / amplitudes.mean(),\n",
    "    }\n",
    "    \n",
    "    return quality_metrics\n",
    "\n",
    "\n",
    "# Display comprehensive results only if focusing was successful\n",
    "if 'focused_radar_data' in locals():\n",
    "    post_focus_metadata = get_focusing_metadata(focuser)\n",
    "    display_focusing_results(focused_radar_data, post_focus_metadata)\n",
    "    \n",
    "    # Analyze focusing quality\n",
    "    print('\\nüéØ Focusing Quality Analysis:')\n",
    "    quality_metrics = analyze_focusing_quality(focused_radar_data)\n",
    "    for metric, value in quality_metrics.items():\n",
    "        print(f'  {metric}: {value:.4f}')\n",
    "else:\n",
    "    print('‚è∏Ô∏è  Focusing not yet completed - run the focusing cell first.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f03257",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_memory(variables_to_keep: Optional[list] = None) -> None:\n",
    "    \"\"\"Clean up memory by deleting large variables and running garbage collection.\n",
    "    \n",
    "    Args:\n",
    "        variables_to_keep (Optional[list]): List of variable names to preserve.\n",
    "    \"\"\"\n",
    "    import gc\n",
    "    \n",
    "    variables_to_keep = variables_to_keep or ['focused_radar_data', 'focuser']\n",
    "    \n",
    "    # Get current globals\n",
    "    current_globals = list(globals().keys())\n",
    "    \n",
    "    # Variables that are safe to delete (large data arrays)\n",
    "    deletable_vars = ['echo', 'metadata', 'ephemeris', 'raw_data']\n",
    "    \n",
    "    deleted_count = 0\n",
    "    for var_name in deletable_vars:\n",
    "        if var_name in current_globals and var_name not in variables_to_keep:\n",
    "            try:\n",
    "                del globals()[var_name]\n",
    "                deleted_count += 1\n",
    "                print(f'  ‚úÖ Deleted variable: {var_name}')\n",
    "            except KeyError:\n",
    "                pass\n",
    "    \n",
    "    # Run garbage collection\n",
    "    collected = gc.collect()\n",
    "    print(f'  üóëÔ∏è  Garbage collector freed {collected} objects')\n",
    "    print(f'  üìù Deleted {deleted_count} large variables')\n",
    "\n",
    "\n",
    "# Final memory usage check\n",
    "print('\\nüíæ Final Memory Usage:')\n",
    "print_memory_usage()\n",
    "\n",
    "print('\\nüßπ Memory Cleanup (optional):')\n",
    "print('Uncomment the line below to free memory from large intermediate variables:')\n",
    "print('# cleanup_memory()')\n",
    "\n",
    "# Uncomment to actually run cleanup\n",
    "# cleanup_memory()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
